{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9401942,"sourceType":"datasetVersion","datasetId":5707438},{"sourceId":9655730,"sourceType":"datasetVersion","datasetId":5898444},{"sourceId":104449,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102},{"sourceId":141778,"sourceType":"modelInstanceVersion","modelInstanceId":120095,"modelId":143319},{"sourceId":145784,"sourceType":"modelInstanceVersion","modelInstanceId":123627,"modelId":146689}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ******************************* 0 ******************************* \n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n!pip install jinja2\n!pip install -U bitsandbytes\n!pip install peft \n!pip install accelerate --upgrade\n!pip install --upgrade transformers\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-01T10:18:30.189966Z","iopub.execute_input":"2024-12-01T10:18:30.193160Z","iopub.status.idle":"2024-12-01T10:19:50.412201Z","shell.execute_reply.started":"2024-12-01T10:18:30.193067Z","shell.execute_reply":"2024-12-01T10:19:50.410632Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2) (2.1.5)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nCollecting accelerate\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.33.0\n    Uninstalling accelerate-0.33.0:\n      Successfully uninstalled accelerate-0.33.0\nSuccessfully installed accelerate-1.1.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed tokenizers-0.20.3 transformers-4.46.3\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n/kaggle/input/rephrased-data/combined_dataset.csv\n/kaggle/input/prefix_model_25/transformers/v1/1/adapter_model.safetensors\n/kaggle/input/prefix_model_25/transformers/v1/1/training_args.bin\n/kaggle/input/prefix_model_25/transformers/v1/1/adapter_config.json\n/kaggle/input/prefix_model_25/transformers/v1/1/README.md\n/kaggle/input/prefix_model_25/transformers/v1/1/tokenizer.json\n/kaggle/input/prefix_model_25/transformers/v1/1/tokenizer_config.json\n/kaggle/input/prefix_model_25/transformers/v1/1/special_tokens_map.json\n/kaggle/input/prefix_model_25/transformers/v1/1/lora/adapter_model.safetensors\n/kaggle/input/prefix_model_25/transformers/v1/1/lora/adapter_config.json\n/kaggle/input/prefix_model_25/transformers/v1/1/lora/README.md\n/kaggle/input/combined-data/combines_dataset.csv\n/kaggle/input/qlora-25/transformers/v1/1/adapter_model.safetensors\n/kaggle/input/qlora-25/transformers/v1/1/training_args.bin\n/kaggle/input/qlora-25/transformers/v1/1/adapter_config.json\n/kaggle/input/qlora-25/transformers/v1/1/README.md\n/kaggle/input/qlora-25/transformers/v1/1/tokenizer.json\n/kaggle/input/qlora-25/transformers/v1/1/tokenizer_config.json\n/kaggle/input/qlora-25/transformers/v1/1/special_tokens_map.json\n/kaggle/input/qlora-25/transformers/v1/1/lora/adapter_model.safetensors\n/kaggle/input/qlora-25/transformers/v1/1/lora/adapter_config.json\n/kaggle/input/qlora-25/transformers/v1/1/lora/README.md\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# # Necessary until transformers package is updated in the Kaggle notebook environment.\n\n\n# import transformers\n# import torch\n\n# torch.backends.cuda.enable_mem_efficient_sdp(False)\n# torch.backends.cuda.enable_flash_sdp(False)\n\n# model = \"/kaggle/input/llama-3.1/transformers/8b/1\"","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Llama 3.1 8B-Instruct","metadata":{}},{"cell_type":"code","source":"# prompt = '''Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\n# The Test consists of three different sections where each section contains 5 questions each of easy, medium and hard difficulty level.\n# Use Bloom's Taxonomy to define the difficulty level of the question.\n# Remember the questions should be from the passage only. \n# '''\n# passage = '''Horticulture Growing vegetables, flowers and fruits for commercial use.\n# Farm System Agriculture or farming can be looked at as a system. The important inputs are\n# seeds, fertilisers, machinery and labour. Some of the operations involved are ploughing, sowing,\n# irrigation, weeding and harvesting. The outputs from the system include crops, wool, dairy and\n# poultry products. Types of Farming Farming is practised in various ways across the world.\n# Depending upon the geographical conditions, demand of produce, labour and level of\n# technology, farming can be classified into two main types. These are subsistence farming and\n# commercial farming. Subsistence Farming This type of farming is practised to meet the needs of\n# the farmer’s family. Traditionally, low levels of technology and household labour are used to\n# produce on small output. Subsistence farming can be further classified as intensive subsistence\n# and primitive subsistence farming. In intensive subsistence agriculture the farmer cultivates a\n# small plot of land using simple tools and more labour. Climate with large number of days with\n# sunshine and fertile soils permit growing of more than one crop annually on the same plot. Rice\n# is the main crop. Other crops include wheat, maize, pulses and oilseeds. Intensive subsistence\n# agriculture is prevalent in the thickly populated areas of the monsoon regions of south,\n# southeast and east Asia. Primitive subsistence agriculture includes shifting cultivation and\n# nomadic herding. Shifting cultivation is practised in the thickly forested areas of Amazon basin,\n# tropical Africa, parts of southeast Asia and Northeast India. These are the areas of heavy\n# rainfall and quick regeneration of vegetation. A plot of land is cleared by felling the trees and\n# burning them. The ashes are then mixed with the soil and crops like maize, yam, potatoes and\n# cassava are grown. After the soil loses its fertility, the land is abandoned and the cultivator\n# moves to a new plot. Shifting cultivation is also known as ‘slash and burn’ agriculture. Nomadic\n# herding is practised in the semi-arid and arid regions of Sahara, Central Asia and some parts of\n# India, like Rajasthan and Jammu and Kashmir. In this type of farming, herdsmen move from\n# place to place with their animals for fodder and water, along defined routes. This type of\n# movement arises in response to climatic constraints and terrain. Sheep, camel, yak and goatsare most commonly reared. They provide milk, meat, wool, hides and other products to the\n# herders and their families.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Necessary until transformers packages is updated in the Kaggle notebook environment.\n# !pip install --upgrade transformers\n\n# import transformers\n# import torch\n\n# torch.backends.cuda.enable_mem_efficient_sdp(False)\n# torch.backends.cuda.enable_flash_sdp(False)\n\n# model_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\n# pipeline = transformers.pipeline(\n#     \"text-generation\",\n#     model=model_id,\n#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n#     device_map=\"auto\",\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompt = '''Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\n# The Test consists of three different sections where each section contains 5 questions each of easy, medium and hard difficulty level.\n# Use Bloom's Taxonomy to define the difficulty level of the question.\n# Remember the questions should be from the passage only. '''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# messages = [\n#     {\"role\": \"system\", \"content\": \"You are an assitant social studies teacher in a High School.\"},\n#     {\"role\": \"user\", \"content\": prompt + passage},\n# ]\n\n# outputs = pipeline(\n#     messages,\n#     max_new_tokens=2048,\n# )\n# print(outputs[0][\"generated_text\"][-1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generated MCQS on ","metadata":{}},{"cell_type":"markdown","source":"\"**Section 1: Recall (Easy Difficulty Level)**\\n\\n1. What is Horticulture?\\n   a) Growing vegetables, flowers and fruits for commercial use\\n   b) Farming for subsistence\\n   c) Farming for commercial use\\n   d) Growing crops for household use\\n\\nAnswer: a) Growing vegetables, flowers and fruits for commercial use\\n\\n2. What are the important inputs in a farm system?\\n   a) Seeds, fertilisers, machinery and labour\\n   b) Seeds, fertilisers, animals and soil\\n   c) Seeds, labour, machinery and water\\n   d) Seeds, fertilisers, animals and tools\\n\\nAnswer: a) Seeds, fertilisers, machinery and labour\\n\\n3. What are the outputs from a farm system?\\n   a) Crops, wool, dairy and poultry products\\n   b) Crops, seeds, fertilisers and labour\\n   c) Crops, animals, tools and water\\n   d) Crops, animals, seeds and soil\\n\\nAnswer: a) Crops, wool, dairy and poultry products\\n\\n4. What is subsistence farming?\\n   a) Farming to meet the needs of the farmer's family\\n   b) Farming for commercial use\\n   c) Farming for household use\\n   d) Farming for export\\n\\nAnswer: a) Farming to meet the needs of the farmer's family\\n\\n5. What is intensive subsistence agriculture?\\n   a) Farming using simple tools and more labour\\n   b) Farming using complex tools and less labour\\n   c) Farming using animals and household labour\\n   d) Farming using machines and commercial labour\\n\\nAnswer: a) Farming using simple tools and more labour\\n\\n\n**Section 2: Analyze (Medium Difficulty Level)**\\n\\n6. What are the geographical conditions that permit intensive subsistence agriculture? (Bloom's Taxonomy: Analyze)\\n   a) Large number of days with sunshine and fertile soils\\n   b) Heavy rainfall and quick regeneration of vegetation\\n   c) Semi-arid and arid regions\\n   d) Thickly populated areas\\n\\nAnswer: a) Large number of days with sunshine and fertile soils\\n\\n7. What are the main crops grown in intensive subsistence agriculture? (Bloom's Taxonomy: Analyze)\\n   a) Rice, wheat, maize, pulses and oilseeds\\n   b) Maize, yam, potatoes and cassava\\n   c) Sheep, camel, yak and goats\\n   d) Cattle, buffalo and horses\\n\\nAnswer: a) Rice, wheat, maize, pulses and oilseeds\\n\\n8. What is shifting cultivation? (Bloom's Taxonomy: Analyze)\\n   a) Farming using simple tools and more labour\\n   b) Farming using animals and household labour\\n   c) Shifting from one plot of land to another\\n   d) Farming using machines and commercial labour\\n\\nAnswer: c) Shifting from one plot of land to another\\n\\n9. What is nomadic herding? (Bloom's Taxonomy: Analyze)\\n   a) Farming using simple tools and more labour\\n   b) Farming using animals and household labour\\n   c) Moving from place to place with animals for fodder and water\\n   d) Farming using machines and commercial labour\\n\\nAnswer: c) Moving from place to place with animals for fodder and water\\n\\n10. What are the products provided by sheep, camel, yak and goats in nomadic herding? (Bloom's Taxonomy: Analyze)\\n    a) Milk, meat, wool, hides and other products\\n    b) Crops, seeds, fertilisers and labour\\n    c) Crops, animals, tools and water\\n    d) Crops, animals, seeds and soil\\n\\nAnswer: a) Milk, meat, wool, hides and other products\\n\\n\n**Section 3: Evaluate (Hard Difficulty Level)**\\n\\n11. What are the limitations of subsistence farming? (Bloom's Taxonomy: Evaluate)\\n    a) Low levels of technology and household labour\\n    b) High levels of technology and commercial labour\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: a) Low levels of technology and household labour\\n\\n12. What are the advantages of intensive subsistence agriculture? (Bloom's Taxonomy: Evaluate)\\n    a) High levels of technology and commercial labour\\n    b) Low levels of technology and household labour\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: c) Large number of days with sunshine and fertile soils\\n\\n13. What are the limitations of shifting cultivation? (Bloom's Taxonomy: Evaluate)\\n    a) Soil loses its fertility and is abandoned\\n    b) Soil retains its fertility and is reused\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: a) Soil loses its fertility and is abandoned\\n\\n14. What are the advantages of nomadic herding? (Bloom's Taxonomy: Evaluate)\\n    a) High levels of technology and commercial labour\\n    b) Low levels of technology and household labour\\n    c) Moving from place to place with animals for fodder and water\\n    d) Farming using machines and commercial labour\\n\\nAnswer: c) Moving from place to place with animals for fodder and water\\n\\n15. What are the geographical conditions that permit nomadic herding? (Bloom's Taxonomy: Evaluate)\\n    a) Large number of days with sunshine and fertile soils\\n    b) Heavy rainfall and quick regeneration of vegetation\\n    c) Semi-arid and arid regions\\n    d) Thickly populated areas\\n\\nAnswer: c) Semi-arid and arid regions\"}","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport bitsandbytes as bnb\nfrom transformers import LlamaForCausalLM, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig\nfrom transformers import AutoTokenizer\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T10:19:50.414651Z","iopub.execute_input":"2024-12-01T10:19:50.414998Z","iopub.status.idle":"2024-12-01T10:20:14.870843Z","shell.execute_reply.started":"2024-12-01T10:19:50.414964Z","shell.execute_reply":"2024-12-01T10:20:14.869823Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:36:10.169197Z","iopub.execute_input":"2024-10-25T07:36:10.169798Z","iopub.status.idle":"2024-10-25T07:36:10.174138Z","shell.execute_reply.started":"2024-10-25T07:36:10.169763Z","shell.execute_reply":"2024-10-25T07:36:10.173108Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# loading dataset","metadata":{}},{"cell_type":"code","source":"# ******************************* 1 ******************************* \n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/combined-data/combines_dataset.csv', encoding='ISO-8859-1')\n# df = pd.read_csv('/kaggle/input/rephrased-data/combined_dataset.csv', encoding='ISO-8859-1')\n","metadata":{"execution":{"iopub.status.busy":"2024-12-01T10:20:14.872298Z","iopub.execute_input":"2024-12-01T10:20:14.872953Z","iopub.status.idle":"2024-12-01T10:20:14.933154Z","shell.execute_reply.started":"2024-12-01T10:20:14.872919Z","shell.execute_reply":"2024-12-01T10:20:14.932011Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the dataset\ndf_old = df\n\n# Drop empty rows\ndf = df_old.dropna(how='all')  # This removes rows where all elements are NaN (empty)\n\n# Print the number of rows after cleaning\nprint(f\"Number of rows after cleaning: {len(df)}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-01T10:20:14.935520Z","iopub.execute_input":"2024-12-01T10:20:14.935880Z","iopub.status.idle":"2024-12-01T10:20:14.945999Z","shell.execute_reply.started":"2024-12-01T10:20:14.935848Z","shell.execute_reply":"2024-12-01T10:20:14.944610Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of rows after cleaning: 143\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ******************************* 1.1 ******************************* \n# Display the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-01T10:20:14.947529Z","iopub.execute_input":"2024-12-01T10:20:14.947876Z","iopub.status.idle":"2024-12-01T10:20:14.977687Z","shell.execute_reply.started":"2024-12-01T10:20:14.947843Z","shell.execute_reply":"2024-12-01T10:20:14.976497Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                             passage  \\\n0  This transformation from a plant to a finished...   \n1  People are a nationâs greatest resource. Nat...   \n2  Have you ever given a thought to the fact that...   \n3  In a small village in Tanzania, Africa, Mamba ...   \n4  Water, electricity, rickshaw, vegetable and te...   \n\n                                                mcqs  \n0  ['## Agriculture and Economic Activities: An M...  \n1  ['## Population Studies Test\\n', '\\n', '**Inst...  \n2  ['## The Journey of Your Notebook: A Social St...  \n3  ['## Social Studies Test: Land as a Resource\\n...  \n4  ['## Agriculture and Farming: An MCQ Test\\n', ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>passage</th>\n      <th>mcqs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>This transformation from a plant to a finished...</td>\n      <td>['## Agriculture and Economic Activities: An M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>People are a nationâs greatest resource. Nat...</td>\n      <td>['## Population Studies Test\\n', '\\n', '**Inst...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Have you ever given a thought to the fact that...</td>\n      <td>['## The Journey of Your Notebook: A Social St...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In a small village in Tanzania, Africa, Mamba ...</td>\n      <td>['## Social Studies Test: Land as a Resource\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Water, electricity, rickshaw, vegetable and te...</td>\n      <td>['## Agriculture and Farming: An MCQ Test\\n', ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ******************************* 1.3 ******************************* \n# Import the splitting function\nfrom sklearn.model_selection import train_test_split\n\n# Split the formatted data into training and testing sets\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Check the lengths of the train and test splits\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Testing set size: {len(test_data)}\")\n\n# Optional: Print the first entry from the training and test set\nprint(f\"First training example: {train_data.iloc[0]}\")\nprint(len(train_data),\"\\n\")\nprint(f\"First testing example: {test_data.iloc[0]}\")\nprint(len(test_data))\n","metadata":{"execution":{"iopub.status.busy":"2024-12-01T10:20:14.979474Z","iopub.execute_input":"2024-12-01T10:20:14.979809Z","iopub.status.idle":"2024-12-01T10:20:15.011038Z","shell.execute_reply.started":"2024-12-01T10:20:14.979777Z","shell.execute_reply":"2024-12-01T10:20:15.009893Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training set size: 114\nTesting set size: 29\nFirst training example: passage    1. Peasants and Agricultural Production The ba...\nmcqs       ['## Peasant Life and Agriculture in Mughal In...\nName: 124, dtype: object\n114 \n\nFirst testing example: passage    1. A Mosaic of Religious Beliefs and Practices...\nmcqs       ['## MCQ Test: A Mosaic of Religious Beliefs\\n...\nName: 117, dtype: object\n29\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-01T10:20:15.012540Z","iopub.execute_input":"2024-12-01T10:20:15.013127Z","iopub.status.idle":"2024-12-01T10:20:15.019406Z","shell.execute_reply.started":"2024-12-01T10:20:15.013077Z","shell.execute_reply":"2024-12-01T10:20:15.018025Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"passages = train_data['passage'].tolist()\nmcqs = train_data['mcqs'].tolist()\n\npassage_test = test_data['passage'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-12-01T10:20:15.020651Z","iopub.execute_input":"2024-12-01T10:20:15.021004Z","iopub.status.idle":"2024-12-01T10:20:15.031249Z","shell.execute_reply.started":"2024-12-01T10:20:15.020971Z","shell.execute_reply":"2024-12-01T10:20:15.029819Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## New Prompt\n<|begin_of_text|>\n<|start_header_id|>MCQs_Generator<|end_header_id|>\nYou are a helpful MCQ Generator and Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\nThe Test consists of three different sections where each section contains 5 questions each of easy, medium and hard difficulty level.\nUse Bloom's Taxonomy to define the difficulty level of the question.\nRemember the questions should be from the passage only. <|eot_id|>\n\n<|start_header_id|>User<|end_header_id|>\npassage<|eot_id|>\n\n<|start_header_id|>MCQs_Generator<|end_header_id|>\nmcqs<|eot_id|>\n\n<|end_of_text|>\n","metadata":{}},{"cell_type":"code","source":"# ******************************* 1.2 ******************************* \ntrain_data = []\n\nfor passage, mcq in zip(passages, mcqs):\n    formatted_entry = f'''\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage: \n{passage} <|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n{mcq}<|eot_id|>\n<|end_of_text|>\n'''\n    train_data.append(formatted_entry)\n\n# print(train_data[72])","metadata":{"execution":{"iopub.status.busy":"2024-12-01T10:20:15.032762Z","iopub.execute_input":"2024-12-01T10:20:15.033084Z","iopub.status.idle":"2024-12-01T10:20:15.049504Z","shell.execute_reply.started":"2024-12-01T10:20:15.033053Z","shell.execute_reply":"2024-12-01T10:20:15.048234Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"len(train_data[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T10:20:58.799119Z","iopub.execute_input":"2024-12-01T10:20:58.800095Z","iopub.status.idle":"2024-12-01T10:20:58.806145Z","shell.execute_reply.started":"2024-12-01T10:20:58.800046Z","shell.execute_reply":"2024-12-01T10:20:58.805122Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"14810"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# ******************************* 1.3 ******************************* \nevaluation_data = []\n\nfor passage in (passage_test):\n    formatted_entry =  f'''\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n{passage} <|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n<|eot_id|>\n<|end_of_text|>\n'''\n\n\n    evaluation_data.append(formatted_entry)\n\n# print(evaluation_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-12-01T08:25:33.980050Z","iopub.execute_input":"2024-12-01T08:25:33.980695Z","iopub.status.idle":"2024-12-01T08:25:33.986851Z","shell.execute_reply.started":"2024-12-01T08:25:33.980660Z","shell.execute_reply":"2024-12-01T08:25:33.985739Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n# Define a BitsAndBytesConfig for 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",  # You can choose \"fp4\" or \"nf4\"\n    bnb_4bit_use_double_quant=True,  # Optional: Use double quantization for better precision\n    bnb_4bit_compute_dtype=\"float16\"  # Set compute to float16 to save memory\n)\n\n# Load the model with the new quantization config\nmodel = LlamaForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,  # Use BitsAndBytesConfig for quantization\n    device_map='auto'  # Automatically map the model to available devices (e.g., GPU)\n)\n\n# Apply LoRA (Low-Rank Adaptation)\npeft_params = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, peft_params)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:36:42.577357Z","iopub.execute_input":"2024-10-25T07:36:42.578097Z","iopub.status.idle":"2024-10-25T07:38:03.879151Z","shell.execute_reply.started":"2024-10-25T07:36:42.578056Z","shell.execute_reply":"2024-10-25T07:38:03.878270Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af279df190904f19a70c5c4a737de1e8"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# ******************************* 2.1 ******************************* \ntokenizer_base = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-3.1/transformers/8b-instruct/2/\")\ntokenizer_base.add_special_tokens({\n    'pad_token': '[PAD]',\n    'additional_special_tokens': ['<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', '<|eom_id|>', '<|eot_id|>', '<|end_of_text|>']\n})\n\n# Tokenize the formatted data\ntokenized_train_data = tokenizer_base(train_data, truncation=True, padding=\"max_length\", max_length=512)\ntokenized_eval_data = tokenizer_base(evaluation_data, truncation=True, padding=\"max_length\", max_length=512)\n\n# ******************************* 3 *******************************\n# Function to format tokenized data for the Trainer\ndef format_data_for_trainer(tokenized_data):\n    formatted_data = []\n    for i in range(len(tokenized_data['input_ids'])):\n        formatted_data.append({\n            'input_ids': tokenized_data['input_ids'][i],\n            'attention_mask': tokenized_data['attention_mask'][i],  # Include attention mask\n            'labels': tokenized_data['input_ids'][i]  # You can modify labels if needed\n        })\n    return formatted_data\n\nformatted_train_data = format_data_for_trainer(tokenized_train_data)\nformatted_eval_data = format_data_for_trainer(tokenized_eval_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:38:08.345874Z","iopub.execute_input":"2024-10-25T07:38:08.346616Z","iopub.status.idle":"2024-10-25T07:38:09.187308Z","shell.execute_reply.started":"2024-10-25T07:38:08.346577Z","shell.execute_reply":"2024-10-25T07:38:09.186464Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(len(tokenizer_base))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:38:10.754140Z","iopub.execute_input":"2024-10-25T07:38:10.754615Z","iopub.status.idle":"2024-10-25T07:38:10.777895Z","shell.execute_reply.started":"2024-10-25T07:38:10.754572Z","shell.execute_reply":"2024-10-25T07:38:10.776920Z"},"trusted":true},"outputs":[{"name":"stdout","text":"128257\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(tokenizer_base.vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:38:11.649961Z","iopub.execute_input":"2024-10-25T07:38:11.650813Z","iopub.status.idle":"2024-10-25T07:38:11.655587Z","shell.execute_reply.started":"2024-10-25T07:38:11.650771Z","shell.execute_reply":"2024-10-25T07:38:11.654599Z"},"trusted":true},"outputs":[{"name":"stdout","text":"128000\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ******************************* 3.1 ******************************* \nimport torch\nprint(torch.cuda.device_count())  \ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:38:12.716335Z","iopub.execute_input":"2024-10-25T07:38:12.717063Z","iopub.status.idle":"2024-10-25T07:38:12.722154Z","shell.execute_reply.started":"2024-10-25T07:38:12.717022Z","shell.execute_reply":"2024-10-25T07:38:12.721081Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"training_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=25,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    save_steps=30,\n    logging_steps=30,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    save_only_model=True,\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer_base, model=model)\n\ntrainer = Trainer(\n    model=model,\n    args=training_params,\n    train_dataset=formatted_train_data,\n    eval_dataset=formatted_eval_data,\n    data_collator=data_collator\n    )\n\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T06:12:22.207273Z","iopub.execute_input":"2024-10-25T06:12:22.208211Z","iopub.status.idle":"2024-10-25T07:02:47.873805Z","shell.execute_reply.started":"2024-10-25T06:12:22.208169Z","shell.execute_reply":"2024-10-25T07:02:47.872865Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241025_061259-tydsudgy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/goenkalokesh-sri-sathya-sai-institute-of-higher-learning/huggingface/runs/tydsudgy' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/goenkalokesh-sri-sathya-sai-institute-of-higher-learning/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/goenkalokesh-sri-sathya-sai-institute-of-higher-learning/huggingface' target=\"_blank\">https://wandb.ai/goenkalokesh-sri-sathya-sai-institute-of-higher-learning/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/goenkalokesh-sri-sathya-sai-institute-of-higher-learning/huggingface/runs/tydsudgy' target=\"_blank\">https://wandb.ai/goenkalokesh-sri-sathya-sai-institute-of-higher-learning/huggingface/runs/tydsudgy</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [700/700 49:26, Epoch 24/25]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>30</td>\n      <td>2.025000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.417200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.342100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.270700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.173100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.059800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.949000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.810000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.710400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.588000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.496100</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.396300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.321500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.265600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.217000</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.184100</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.146000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.123200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.106800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.093700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.074400</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.072400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=700, training_loss=0.5976629310846329, metrics={'train_runtime': 3023.8606, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.231, 'total_flos': 6.47888652730368e+16, 'train_loss': 0.5976629310846329, 'epoch': 24.56140350877193})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# # ******************************* 4 ******************************* \n# # Define training arguments for multi-GPU and FP16\n# training_args = TrainingArguments(\n#     output_dir='./results_1',\n#     num_train_epochs=20,\n#     per_device_train_batch_size=2,\n#     per_device_eval_batch_size=2,\n#     save_steps=500,\n#     save_total_limit=1,\n#     logging_dir='./logs',\n#     report_to='none',          # optional: to avoid logging issues with multiple GPUs\n#     ddp_find_unused_parameters=True,  # required for multi-GPU\n#     fp16=True,\n# )\n\n# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer_base, model=model)\n\n# # Initialize the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=formatted_train_data,\n#     eval_dataset=formatted_eval_data,\n#     data_collator=data_collator\n#     )\n\n# # Fine-tune the model\n# trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(trainer.model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the fine-tuned Model","metadata":{}},{"cell_type":"code","source":"# Save the model\ntrainer.save_model(\"./New_QLoRA_model\")  # Saves the model (LoRa weights will be part of it)\n\n# Save the tokenizer\ntokenizer_base.save_pretrained(\"./New_QLoRA_model\")\n\n# Save the LoRa adapter (LoRa-specific weights)\nmodel.save_pretrained(\"./New_QLoRA_model/lora\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to your model directory\nmodel_dir = '/kaggle/working/New_QLoRA_model'\n\n# Output zip file path\nzip_filename = '/kaggle/working/New_QLoRA_model.zip'\n\n# Zipping the directory\nshutil.make_archive(zip_filename.replace('.zip', ''), 'zip', model_dir)\n\nprint(\"Model zipped successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing the model","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:32:00.319231Z","iopub.execute_input":"2024-10-25T07:32:00.320122Z","iopub.status.idle":"2024-10-25T07:32:15.006068Z","shell.execute_reply.started":"2024-10-25T07:32:00.320078Z","shell.execute_reply":"2024-10-25T07:32:15.005132Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:32:15.007883Z","iopub.execute_input":"2024-10-25T07:32:15.008197Z","iopub.status.idle":"2024-10-25T07:32:27.902803Z","shell.execute_reply.started":"2024-10-25T07:32:15.008163Z","shell.execute_reply":"2024-10-25T07:32:27.901658Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:32:27.904558Z","iopub.execute_input":"2024-10-25T07:32:27.904967Z","iopub.status.idle":"2024-10-25T07:32:44.973595Z","shell.execute_reply.started":"2024-10-25T07:32:27.904922Z","shell.execute_reply":"2024-10-25T07:32:44.972220Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import LlamaForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom peft import PeftModel\n\n# Define a BitsAndBytesConfig for 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",  # Choose \"nf4\" for better precision\n    bnb_4bit_use_double_quant=True,  # Optional: Use double quantization\n    bnb_4bit_compute_dtype=\"float16\"  # Set compute to float16 to save memory\n)\n\n\nfinetuned_model_path = \"/kaggle/input/qlora-25/transformers/v1/1\"  \n\n# Load the tokenizer used for fine-tuning\n# tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\ntokenizer = tokenizer_base\nprint(\"0\")\n# Load the model with quantization config\nmodel = LlamaForCausalLM.from_pretrained(\n    finetuned_model_path,\n    ignore_mismatched_sizes=True,\n    quantization_config=quantization_config,  \n    device_map=\"auto\" \n)\nprint(\"1\")\n\n# Now load the LoRA adapter (if needed)\nlora_adapter_path = \"/kaggle/input/qlora-25/transformers/v1/1/lora\"\ntuned_model = PeftModel.from_pretrained(model, lora_adapter_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:38:40.551125Z","iopub.execute_input":"2024-10-25T07:38:40.551507Z","iopub.status.idle":"2024-10-25T07:39:00.336501Z","shell.execute_reply.started":"2024-10-25T07:38:40.551473Z","shell.execute_reply":"2024-10-25T07:39:00.335684Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b704ebc62dad499d9ad2e659bfab14e7"}},"metadata":{}},{"name":"stdout","text":"1\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"passage = '''Nestled in the heart of Amritsar, Punjab, the Golden Temple stands as a beacon of spirituality, unity, and service. Floating like a shimmering mirage of gold on the tranquil waters of Amrit Sarovar, the temple is a place of worship and a symbol of Sikh values such as equality, service, and community. It owes its name to the astonishing 400 kilograms of pure gold leaf adorning the dome.\nAs you approach the gurdwara, also known as Harmandir Sahib, you are struck by its majestic golden dome shimmering in the sunlight, drawing you closer to its sacred embrace. The name, Shri Harmandir Sahib, however, comes from ‘Harmandir’ derived from ‘Hari’, signifying God, and ‘mandir’, meaning temple. The addition of ‘Sahib’ to its name denotes reverence and respect within Sikh tradition.\nHistory whispers through the marble walls of the Golden Temple, telling tales of devotion and resilience. While the temple was founded by Guru Ramdas Sahib, the 4th of 10 Sikh gurus, the construction of the temple and its pool was continued by Guru Arjan Dev, the fifth Sikh Guru, in 1588.\nThe temple has witnessed centuries of faith, turmoil, and triumph. The intricate architecture, a fusion of Islamic and Hindu styles, reflects the inclusive ethos of Sikhism, inviting people from all religions come to seek solace. The temple has been renovated many times, adding features such as the marble inlays along the floor. Maharaja Ranjit Singh, founder of the Sikh Empire of India (1799-1849) had the temple’s upper floors covered in 750 kilos of pure gold.\nIts golden dome, exquisite architecture, and serene surroundings attract millions of visitors worldwide every year, making it one of the most visited religious sites in the world. But beyond its stunning beauty, the Golden Temple is also a symbol of Sikh philosophy, which emphasises the equality of all people, regardless of caste, creed, or gender. Sikhs all over the world pray in their Ardas daily, wishing to pay obeisance at Sri Harmandir Sahib (Golden Temple).\nOne visit never feels enough to experience the emotion of the Golden Temple, but you can try. Plan your visit to coincide with the vibrant festivities hosted at the Golden Temple, such as Baisakhi and Diwali, to experience the Golden Temple at its most vibrant.\nA sacred haven of spirituality and architecture\nStep through the ornate entrance gates, with enchanting interiors decorated with intricate frescos and floral motifs. Verses from the Sikh scripture Etched in gold lettering grace the arches. A sense of reverence washes over you as you gaze upon the Amrit Sarovar (Pool of Nectar), the tranquil sacred tank surrounding the temple. The waters of the Sarovar are believed to possess healing properties, inviting pilgrims to cleanse their souls and renew their spirits in its pristine embrace. The shimmering reflection of the gold-encrusted dome in the clear water of the Amrit Sarovar greets visitors who enter from the north gate, the most impressive of all four entries. Walking around the marble pathway surrounding the pool is the best way to take it all in. Devotional music, bathing pilgrims, golden carp and meditating devotees add to the atmosphere.\nThe complex has various structures surrounding the main sanctum and the adjacent water body. Among these, the Akal Takht holds prominence, representing one of the five seats of power in Sikhism. A museum, a clock tower, and the heartwarming community kitchen or Langar, complete the ensemble of this sacred haven.\nFestivals like Vaisakhi, Guru Nanak’s birthday, Guru Teg Bahadur’s martyrdom day, and Guru Ram Das’s birthday are celebrated passionately. During Diwali, the Golden Temple illuminates with earthen lamps or diyas, creating a spectacle of light.\nWhere tranquillity meets grandeur\nInside the temple, the Guru Granth Sahib, the eternal Guru of Sikhism, is enshrined in the Darbar Sahib, the central worship hall. The hymns and prayers reverberating through the halls echoed the timeless teachings of the Gurus. As the line to visit the inner sanctum (Darbar Sahib), where the holy book of the Sikhs, the “Guru Granth Sahib,” is kept is long, visits are best scheduled for the late afternoon and early evening. The illuminated temple complex is a stunning sight, and you can end the day at the temple’s Langar (community kitchen), where the aroma of freshly cooked food tantalises you.'''","metadata":{"execution":{"iopub.status.busy":"2024-12-01T08:25:51.463987Z","iopub.execute_input":"2024-12-01T08:25:51.464357Z","iopub.status.idle":"2024-12-01T08:25:51.471306Z","shell.execute_reply.started":"2024-12-01T08:25:51.464328Z","shell.execute_reply":"2024-12-01T08:25:51.470177Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"passage_2 = '''\nGlobal warming refers to the increase in the planet’s overall average temperature in recent decades. Natural processes have always affected Earth’s temperature and climate, but more recently, the planet’s temperature and climate have changed at a higher pace than nature alone can explain. These rapid changes are due to human activities and the widespread use of fossil fuels for energy.\nFossil fuels include coal, oil and natural gas. Burning fossil fuels causes what is known as the “greenhouse effect” in Earth’s atmosphere. The greenhouse effect happens when the sun’s rays penetrate the atmosphere, and the Earth’s surface reflects that heat. Some of the gasses in the atmosphere then trap heat over Earth. Gasses emitted by the burning of fossil fuels are very good at trapping heat and preventing it from leaving the atmosphere. These greenhouse gasses are carbon dioxide, methane, nitrous oxide, chlorofluorocarbons and water vapor. The excess heat in the atmosphere has caused the planet’s average global temperature to rise over time, otherwise known as global warming.\nThe Industrial Revolution, beginning in the mid-18th century, led to the start of an anthropogenic (human-caused) rise in greenhouse gas emissions from Europe and the United States. The invention of the coal-fired steam engine introduced coal as a major source of energy. Soon it was heating homes and fueling machines in factories.\nSince that time, the burning of fossil fuels has steadily increased. Today, many countries around the world use fossil fuels to generate energy for electricity, heat and transportation. Emissions of greenhouse gasses have skyrocketed in the last 100 years, and especially since the 1980s. This has accelerated the rise in Earth’s temperature.\nGlobal warming has presented humans with another issue: climate change. People often use the terms “global warming” and “climate change” interchangeably, but they are different. Global warming refers to Earth’s rising average temperature, while climate change refers to changes in weather patterns and growing seasons around the world. Global warming causes climate change, which poses a serious threat to life on Earth.\nHumans are feeling the impact of global warming around the world as climate change brings intense droughts, wildfires and extreme storms with heavier rainfall. Higher temperatures are altering ecosystems, forcing animals to migrate to cooler places to survive. Scientists predict that, if nothing is done to lower global temperatures, many species will go extinct.\nThe ocean is also warming, and glaciers, ice caps and ice sheets are melting. This is causing sea levels to rise, creating flooding problems for many people who live on islands and in coastal communities.\nCorals have been a symbol of the consequences of a warmer ocean. Many coral reefs—home to thousands of species of fish and other organisms—are dying. National Geographic Explorer Shireen Rahimi is an underwater storyteller who focuses her lens on the impact of global warming on tropical coral reefs. Her images capture humans’ relationships to the changing seas in the South Pacific, the Coral Triangle, and the Caribbean. Rahimi is dedicated to telling personal stories that encourage environmental action.\nCountries around the world are trying to lower greenhouse gas emissions to slow global warming. In 2015, nearly 200 countries signed the Paris Agreement at a United Nations Climate Change conference. The international treaty tasks each country with lowering greenhouse gas emissions. The goal is to slow the pace of global warming and prevent Earth’s temperature from rising 2°C (3.6°F) above pre-industrial temperatures.\n'''","metadata":{"execution":{"iopub.status.busy":"2024-12-01T08:25:52.105827Z","iopub.execute_input":"2024-12-01T08:25:52.106678Z","iopub.status.idle":"2024-12-01T08:25:52.113782Z","shell.execute_reply.started":"2024-12-01T08:25:52.106636Z","shell.execute_reply":"2024-12-01T08:25:52.112576Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"len(passage)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:29:09.737694Z","iopub.execute_input":"2024-12-01T08:29:09.738535Z","iopub.status.idle":"2024-12-01T08:29:09.746001Z","shell.execute_reply.started":"2024-12-01T08:29:09.738480Z","shell.execute_reply":"2024-12-01T08:29:09.744776Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"4369"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# prompt = '''From now on you are an assistant teacher in a high school. Today you have to conduct an MCQ-based test. \n# For the test generate 15 MCQs from the following passage with the answer key at the last. \n# The Test consists of three different sections and each section contains 5 questions each of easy, medium and hard difficulty levels. \n# Use Bloom's Taxonomy to define the difficulty level of the question. \n# Easy Questions require a simple recall of facts or basic concepts.\n# Medium Questions require understanding, application, or analysis of information.\n# Hard Questions require synthesis or evaluation of information.\n# Ensure that two consecutive questions should not have the same option as the answer.\n# If so, reorder the questions. Also, ensure that options 'B' and 'C' are more likely to be the correct answers in all the sections.\n# Please remember that the questions should be from the passage only.\n# '''\n\nprompt = '''\nYou are a helpful Multiple Choice Questions (MCQs) Generator and Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\nFollow these rules:\n- The test consists of 3 sections (easy, medium, hard).\n- Each section contains 5 MCQs.\n- Questions should vary in difficulty using Bloom’s Taxonomy.\n- Provide the correct answer for each question in an answer key.\n- Ensure the questions are directly derived from the passage. \n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(passage)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:26:21.448371Z","iopub.execute_input":"2024-12-01T08:26:21.449118Z","iopub.status.idle":"2024-12-01T08:26:21.455007Z","shell.execute_reply.started":"2024-12-01T08:26:21.449081Z","shell.execute_reply":"2024-12-01T08:26:21.454001Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"4369"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"formatted_entry =  f'''\n<|begin_of_text|>\n<|start_header_id|>System<|end_header_id|>\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. \nYour goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>User<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. \nFollow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n{passage}\n<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n<|eot_id|>\n<|end_of_text|>\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:39:05.061043Z","iopub.execute_input":"2024-10-25T07:39:05.061604Z","iopub.status.idle":"2024-10-25T07:39:05.066988Z","shell.execute_reply.started":"2024-10-25T07:39:05.061557Z","shell.execute_reply":"2024-10-25T07:39:05.066065Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"formatted_entry_2 =  f'''\n<|begin_of_text|>\n<|start_header_id|>System<|end_header_id|>\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>User<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n{passage_2}\n<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n<|eot_id|>\n<|end_of_text|>\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:02:47.921746Z","iopub.execute_input":"2024-10-25T07:02:47.922103Z","iopub.status.idle":"2024-10-25T07:02:47.933222Z","shell.execute_reply.started":"2024-10-25T07:02:47.922062Z","shell.execute_reply":"2024-10-25T07:02:47.932273Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# testing without downloading","metadata":{}},{"cell_type":"markdown","source":"#### 2048","metadata":{}},{"cell_type":"code","source":"# Define input text\ninput_text = formatted_entry\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=2048)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:02:47.934340Z","iopub.execute_input":"2024-10-25T07:02:47.934743Z","iopub.status.idle":"2024-10-25T07:04:29.288495Z","shell.execute_reply.started":"2024-10-25T07:02:47.934701Z","shell.execute_reply":"2024-10-25T07:04:29.287582Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n\nSystem\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. \nYour goal is to generate clear, well-structured MCQs with answers.\n\nUser\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. \nFollow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\nNestled in the heart of Amritsar, Punjab, the Golden Temple stands as a beacon of spirituality, unity, and service. Floating like a shimmering mirage of gold on the tranquil waters of Amrit Sarovar, the temple is a place of worship and a symbol of Sikh values such as equality, service, and community. It owes its name to the astonishing 400 kilograms of pure gold leaf adorning the dome.\nAs you approach the gurdwara, also known as Harmandir Sahib, you are struck by its majestic golden dome shimmering in the sunlight, drawing you closer to its sacred embrace. The name, Shri Harmandir Sahib, however, comes from ‘Harmandir’ derived from ‘Hari’, signifying God, and ‘mandir’, meaning temple. The addition of ‘Sahib’ to its name denotes reverence and respect within Sikh tradition.\nHistory whispers through the marble walls of the Golden Temple, telling tales of devotion and resilience. While the temple was founded by Guru Ramdas Sahib, the 4th of 10 Sikh gurus, the construction of the temple and its pool was continued by Guru Arjan Dev, the fifth Sikh Guru, in 1588.\nThe temple has witnessed centuries of faith, turmoil, and triumph. The intricate architecture, a fusion of Islamic and Hindu styles, reflects the inclusive ethos of Sikhism, inviting people from all religions come to seek solace. The temple has been renovated many times, adding features such as the marble inlays along the floor. Maharaja Ranjit Singh, founder of the Sikh Empire of India (1799-1849) had the temple’s upper floors covered in 750 kilos of pure gold.\nIts golden dome, exquisite architecture, and serene surroundings attract millions of visitors worldwide every year, making it one of the most visited religious sites in the world. But beyond its stunning beauty, the Golden Temple is also a symbol of Sikh philosophy, which emphasises the equality of all people, regardless of caste, creed, or gender. Sikhs all over the world pray in their Ardas daily, wishing to pay obeisance at Sri Harmandir Sahib (Golden Temple).\nOne visit never feels enough to experience the emotion of the Golden Temple, but you can try. Plan your visit to coincide with the vibrant festivities hosted at the Golden Temple, such as Baisakhi and Diwali, to experience the Golden Temple at its most vibrant.\nA sacred haven of spirituality and architecture\nStep through the ornate entrance gates, with enchanting interiors decorated with intricate frescos and floral motifs. Verses from the Sikh scripture Etched in gold lettering grace the arches. A sense of reverence washes over you as you gaze upon the Amrit Sarovar (Pool of Nectar), the tranquil sacred tank surrounding the temple. The waters of the Sarovar are believed to possess healing properties, inviting pilgrims to cleanse their souls and renew their spirits in its pristine embrace. The shimmering reflection of the gold-encrusted dome in the clear water of the Amrit Sarovar greets visitors who enter from the north gate, the most impressive of all four entries. Walking around the marble pathway surrounding the pool is the best way to take it all in. Devotional music, bathing pilgrims, golden carp and meditating devotees add to the atmosphere.\nThe complex has various structures surrounding the main sanctum and the adjacent water body. Among these, the Akal Takht holds prominence, representing one of the five seats of power in Sikhism. A museum, a clock tower, and the heartwarming community kitchen or Langar, complete the ensemble of this sacred haven.\nFestivals like Vaisakhi, Guru Nanak’s birthday, Guru Teg Bahadur’s martyrdom day, and Guru Ram Das’s birthday are celebrated passionately. During Diwali, the Golden Temple illuminates with earthen lamps or diyas, creating a spectacle of light.\nWhere tranquillity meets grandeur\nInside the temple, the Guru Granth Sahib, the eternal Guru of Sikhism, is enshrined in the Darbar Sahib, the central worship hall. The hymns and prayers reverberating through the halls echoed the timeless teachings of the Gurus. As the line to visit the inner sanctum (Darbar Sahib), where the holy book of the Sikhs, the “Guru Granth Sahib,” is kept is long, visits are best scheduled for the late afternoon and early evening. The illuminated temple complex is a stunning sight, and you can end the day at the temple’s Langar (community kitchen), where the aroma of freshly cooked food tantalises you.\n\nassistant\n\n\nｊocument mpfrGenerator-Version: 2.4.1\nProducer: TEx MacDonald\nSpecial-output: CBSE-UMD\nInput-format: html\nOutput-format: html\nRequest-Type: html\nRequest-URI: \nResponse-Body: \nResponse-Status: 200 OK\nTitle: NCERT- Based Multiple Choice Questions (MCQs) Generator \nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. \nFollow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\nThe land of Punjab is as much a part of the Sikh heritage as it is a part of its soil and people.\nPunjab is a major producer of food grains in the country. It is aptly called the ‘Granary of\nIndia’. The fertile alluvial soil and favourable climate in the plains made agriculture a primary\nactivity from very early times. Agricultural production increased manifold with the green revolution\nintroduced in the 1960s. It was mainly concentrated in the Punjab plain. Farmers in the hilly region\nof Punjab faced difficulties in cultivating their land. However, they developed unique varieties of\nwheat and other crops which were well suited to their terrain. Despite being a prosperous region,\nfarmers in many parts of Punjab face problems such as low prices for their produce, high\npregnant with hope women farming costs, debt and suicide. In hilly Punjab, large corporations\nresponded to the crisis in agriculture by setting up food processing units. This helped local\nproducers to earn better prices for their produce. Women were actively involved in this new\nbusiness. They worked as micro finance providers, helped document the history of their village\ngathers information on their land ownership patterns and advocates for their rights. Through\nagriculture and the processing of agricultural produce, women in this region have empowered\nthemselves and asserted their rights. The land of Punjab is as much a part of the Sikh heritage\nas it is a part of its soil and people. Punjab is a major producer of food grains in the country. It is\naptly called the ‘Granary of India’. The fertile alluvial soil and favourable climate in the plains made\nastronomical production a primary activity from very early times. Agricultural production increased\nmanifold with the green revolution introduced in the 1960s. It was mainly concentrated in the Punjab\nplain. Farmers in the hilly region of Punjab faced difficulties in cultivating their land. However, they\ndeveloped unique varieties of wheat and other crops which were well suited to their terrain. Despite\nbeing a prosperous region, farm problems persist in Punjab. Farmers in this region face problems\nsuch as low prices for their produce, high pricing costs, debt and suicide. In hilly Punjab, large\ncorporations responded to the crisis in agriculture by setting up food processing units. This helped\nlocal producers to earn better prices for their produce. Women were actively involved in this new\nbusiness. They worked as micro finance providers, helped document the history of their village,\ngathers information on their land ownership patterns and advocates for their rights. Through\nagriculture and the processing of agricultural produce, women in this region have empowered\nthemselves and asserted their rights. The alluvial soil of the plains makes agriculture a primary\nactivity from very early times. Agricultural production increased manifold with the green revolution\nintroduced in the 1960s. It was mainly concentrated in the plains. Being a prosperous region, farm\nproblems still persist in the plains. Farmers in the hilly region of Punjab face difficulties in\ncultivating their land. Unique varieties of wheat and other crops have been developed by farmers in\nthe hilly region which are well suited to their terrain. In the alluvial plains of Punjab, agriculture is a\nprimary activity from very early times. With the green revolution introducing new agricultural\ntechnologies in 1960s, agricultural production increased manifold in the plains. Being a prosperous\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Define input text\ninput_text = formatted_entry_2\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=2048)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:04:29.289727Z","iopub.execute_input":"2024-10-25T07:04:29.290076Z","iopub.status.idle":"2024-10-25T07:06:44.808096Z","shell.execute_reply.started":"2024-10-25T07:04:29.290042Z","shell.execute_reply":"2024-10-25T07:06:44.807146Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nSystem\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n\nUser\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n\nGlobal warming refers to the increase in the planet’s overall average temperature in recent decades. Natural processes have always affected Earth’s temperature and climate, but more recently, the planet’s temperature and climate have changed at a higher pace than nature alone can explain. These rapid changes are due to human activities and the widespread use of fossil fuels for energy.\nFossil fuels include coal, oil and natural gas. Burning fossil fuels causes what is known as the “greenhouse effect” in Earth’s atmosphere. The greenhouse effect happens when the sun’s rays penetrate the atmosphere, and the Earth’s surface reflects that heat. Some of the gasses in the atmosphere then trap heat over Earth. Gasses emitted by the burning of fossil fuels are very good at trapping heat and preventing it from leaving the atmosphere. These greenhouse gasses are carbon dioxide, methane, nitrous oxide, chlorofluorocarbons and water vapor. The excess heat in the atmosphere has caused the planet’s average global temperature to rise over time, otherwise known as global warming.\nThe Industrial Revolution, beginning in the mid-18th century, led to the start of an anthropogenic (human-caused) rise in greenhouse gas emissions from Europe and the United States. The invention of the coal-fired steam engine introduced coal as a major source of energy. Soon it was heating homes and fueling machines in factories.\nSince that time, the burning of fossil fuels has steadily increased. Today, many countries around the world use fossil fuels to generate energy for electricity, heat and transportation. Emissions of greenhouse gasses have skyrocketed in the last 100 years, and especially since the 1980s. This has accelerated the rise in Earth’s temperature.\nGlobal warming has presented humans with another issue: climate change. People often use the terms “global warming” and “climate change” interchangeably, but they are different. Global warming refers to Earth’s rising average temperature, while climate change refers to changes in weather patterns and growing seasons around the world. Global warming causes climate change, which poses a serious threat to life on Earth.\nHumans are feeling the impact of global warming around the world as climate change brings intense droughts, wildfires and extreme storms with heavier rainfall. Higher temperatures are altering ecosystems, forcing animals to migrate to cooler places to survive. Scientists predict that, if nothing is done to lower global temperatures, many species will go extinct.\nThe ocean is also warming, and glaciers, ice caps and ice sheets are melting. This is causing sea levels to rise, creating flooding problems for many people who live on islands and in coastal communities.\nCorals have been a symbol of the consequences of a warmer ocean. Many coral reefs—home to thousands of species of fish and other organisms—are dying. National Geographic Explorer Shireen Rahimi is an underwater storyteller who focuses her lens on the impact of global warming on tropical coral reefs. Her images capture humans’ relationships to the changing seas in the South Pacific, the Coral Triangle, and the Caribbean. Rahimi is dedicated to telling personal stories that encourage environmental action.\nCountries around the world are trying to lower greenhouse gas emissions to slow global warming. In 2015, nearly 200 countries signed the Paris Agreement at a United Nations Climate Change conference. The international treaty tasks each country with lowering greenhouse gas emissions. The goal is to slow the pace of global warming and prevent Earth’s temperature from rising 2°C (3.6°F) above pre-industrial temperatures.\n\n\nassistant\n\n\nｊGenerator-Version: 2.0\nResponse-Time: 23 ms\nTotal-Time: 23 ms\nRequest-Type: MCQ-Generation\nUser-Agent: Mozilla/5.0\nUser-Language: en-US\nUser-IP: 72.211.155.93\nMultiple-Choice-Question: Use the following passage to generate a total of 15 MCQs, with an answer key. Follow specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloomâs Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage: \nYou have already learnt that the climate of a place is determined by the amount of solar\nradiation received at its surface. Though the Sunâs energy output remains constant, the\namount of solar radiation that reaches the Earthâs surface is variable. This variability is\nprimarily due to the changing amount of solar radiation that is able to pass through the\natmosphere. Now, let us look at how this happens. (i) Role of Solar Radiation : The\nenergy from the Sun is the source of all energy on Earth. The climate models rely heavily on\nour understanding of the variations in the solar radiation that reaches the Earthâs surface.\n(T) Though the Sunâs energy output remains constant, the amount of solar radiation that\nreaches the Earthâs surface is variable. This variability is primarily due to the changing\namount of solar radiation that is able to pass through the atmosphere. (ii) Role of Atmospheric\nVariables in Climate Modeling : Variables in climate models are quantities that are used to\ncompute the response of the climate system to changes in radiative forcing. For example,\nmodels might examine how a change in solar radiation affects surface temperature, or how\nit alters the strength of atmospheric circulation. (iii) Forcing Functions : Forcing functions are\nchanges in radiative forcing that drive the response of the climate system. They include\nchanges in solar radiation, greenhouse gases, aerosols, land use and other factors. (iv) External\nForcing versus Internal Variability : Climate variability refers to the fluctuations in the climate\nthat occur over various time scales. While internal variability is associated with variations in\nthe climate variables that occur due to changes in the atmospheric and oceanic variables\nthemselves, external forcing refers to variations due to changes in radiative forcing. (v) Natural\nvs. Anthropogenic Forcing : Natural forcing refers to changes in radiative forcing that occur\nnaturally without human influence. Anthropogenic forcing refers to changes in radiative\nforcing that are caused by human activities. (vi) External Forcing versus Internal Variability : Climate\nvariable refers to the fluctuations in the climate that occur over various time scales. While\ninternal variability is associated with variations in the climate variables that occur due to\nchanges in the atmospheric and oceanic variables themselves, external forcing refers to\nvariations due to changes in radiative forcing. (vii) Natural vs. Anthropogenic Forcing : Natural\nforcing refers to changes in radiative forcing that occur naturally without human influence.\nAnthropogenic forcing refers to changes in radiative forcing that are caused by human\nactivities. (viii) External vs. Internal Forcing : External forcing refers to changes in radiative\nforcing that occur from outside the climate system. Internal forcing occurs from within the\nclimate system. (ix) External vs. Internal Radiation Changes : External radiation changes refer\nto changes in the amount of radiation energy entering or leaving the climate system. Internal\nradiation changes occur due to changes in the radiation budget within the climate system.\n(x) External vs. Internal Variables in Climate Modeling : Variables in climate models are\nquantities that are used to compute the response of the climate system to changes in radiative\nforcing. For example, models might examine how a change in solar radiation affects surface\ntemperature, or how it alters the strength of atmospheric circulation. (xi) Forcing Functions : Forcing\nfunctions are changes in radiative forcing that drive the response of the climate system. They\ninclude changes in solar radiation, greenhouse gases, aerosols, land use and other factors.\n(xii) External vs. Internal Forcing Functions : External forcing functions refer to changes in radiative\nforcing that occur from outside the climate system. They include changes in solar radiation,\ngreenhouse gases, aerosols, volcanic eruptions, changes in solar radiation variability, land use\nchanges and other external factors. Internal forcing functions occur within the climate system\nitself. Changes in ocean and land temperatures, changes in soil moisture content, and changes\nin volcanic eruptions occurring within the climate system are examples of internal forcing\nfunctions. (xiii) Natural vs. Anthropogenic Forcing : Natural forcing refers to changes in radiative\nforcing that occur naturally without human influence. Anthropogenic forcing refers to changes\nin radiative forcing that are caused by human activities. Changes in greenhouse gases\nreleased by the burning of fossil fuels and deforestation are examples of anthropogenic\nforcing. Changes in greenhouse gases released by volcanic eruptions and variations in\n terrestrial vegetation occurrence and distribution are examples of natural forcing. (xiv) External\nForcing versus Internal Variability : Climate variability refers to the fluctuations in the climate\nthat occur over various time scales. While internal variability is associated with variations in\nthe climate variables that occur due to changes in the atmospheric and oceanic variables\nthemselves, external forcing refers\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Define input text\ninput_text = evaluation_data[18]\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=3000)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(evaluation_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:27:02.001582Z","iopub.execute_input":"2024-12-01T08:27:02.002436Z","iopub.status.idle":"2024-12-01T08:27:02.008978Z","shell.execute_reply.started":"2024-12-01T08:27:02.002395Z","shell.execute_reply":"2024-12-01T08:27:02.007843Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"8492"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"len(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:28:20.480150Z","iopub.execute_input":"2024-12-01T08:28:20.481226Z","iopub.status.idle":"2024-12-01T08:28:20.971783Z","shell.execute_reply.started":"2024-12-01T08:28:20.481155Z","shell.execute_reply":"2024-12-01T08:28:20.970093Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mgenerated_text\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'generated_text' is not defined"],"ename":"NameError","evalue":"name 'generated_text' is not defined","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"# Define input text\ninput_text = evaluation_data[18]\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=4096)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing with downloading","metadata":{}},{"cell_type":"code","source":"input_text = formatted_entry\n# input_text = evaluation_data[18]\ninputs = tokenizer_base(input_text, return_tensors=\"pt\").to(model.device)\n\n# Generate text\nwith torch.no_grad():\n    outputs = tuned_model.generate(**inputs, max_length=2048)  \n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:41:25.343069Z","iopub.execute_input":"2024-10-25T07:41:25.343955Z","iopub.status.idle":"2024-10-25T07:43:01.401725Z","shell.execute_reply.started":"2024-10-25T07:41:25.343910Z","shell.execute_reply":"2024-10-25T07:43:01.400545Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nSystem\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. \nYour goal is to generate clear, well-structured MCQs with answers.\n\nUser\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. \nFollow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\nNestled in the heart of Amritsar, Punjab, the Golden Temple stands as a beacon of spirituality, unity, and service. Floating like a shimmering mirage of gold on the tranquil waters of Amrit Sarovar, the temple is a place of worship and a symbol of Sikh values such as equality, service, and community. It owes its name to the astonishing 400 kilograms of pure gold leaf adorning the dome.\nAs you approach the gurdwara, also known as Harmandir Sahib, you are struck by its majestic golden dome shimmering in the sunlight, drawing you closer to its sacred embrace. The name, Shri Harmandir Sahib, however, comes from ‘Harmandir’ derived from ‘Hari’, signifying God, and ‘mandir’, meaning temple. The addition of ‘Sahib’ to its name denotes reverence and respect within Sikh tradition.\nHistory whispers through the marble walls of the Golden Temple, telling tales of devotion and resilience. While the temple was founded by Guru Ramdas Sahib, the 4th of 10 Sikh gurus, the construction of the temple and its pool was continued by Guru Arjan Dev, the fifth Sikh Guru, in 1588.\nThe temple has witnessed centuries of faith, turmoil, and triumph. The intricate architecture, a fusion of Islamic and Hindu styles, reflects the inclusive ethos of Sikhism, inviting people from all religions come to seek solace. The temple has been renovated many times, adding features such as the marble inlays along the floor. Maharaja Ranjit Singh, founder of the Sikh Empire of India (1799-1849) had the temple’s upper floors covered in 750 kilos of pure gold.\nIts golden dome, exquisite architecture, and serene surroundings attract millions of visitors worldwide every year, making it one of the most visited religious sites in the world. But beyond its stunning beauty, the Golden Temple is also a symbol of Sikh philosophy, which emphasises the equality of all people, regardless of caste, creed, or gender. Sikhs all over the world pray in their Ardas daily, wishing to pay obeisance at Sri Harmandir Sahib (Golden Temple).\nOne visit never feels enough to experience the emotion of the Golden Temple, but you can try. Plan your visit to coincide with the vibrant festivities hosted at the Golden Temple, such as Baisakhi and Diwali, to experience the Golden Temple at its most vibrant.\nA sacred haven of spirituality and architecture\nStep through the ornate entrance gates, with enchanting interiors decorated with intricate frescos and floral motifs. Verses from the Sikh scripture Etched in gold lettering grace the arches. A sense of reverence washes over you as you gaze upon the Amrit Sarovar (Pool of Nectar), the tranquil sacred tank surrounding the temple. The waters of the Sarovar are believed to possess healing properties, inviting pilgrims to cleanse their souls and renew their spirits in its pristine embrace. The shimmering reflection of the gold-encrusted dome in the clear water of the Amrit Sarovar greets visitors who enter from the north gate, the most impressive of all four entries. Walking around the marble pathway surrounding the pool is the best way to take it all in. Devotional music, bathing pilgrims, golden carp and meditating devotees add to the atmosphere.\nThe complex has various structures surrounding the main sanctum and the adjacent water body. Among these, the Akal Takht holds prominence, representing one of the five seats of power in Sikhism. A museum, a clock tower, and the heartwarming community kitchen or Langar, complete the ensemble of this sacred haven.\nFestivals like Vaisakhi, Guru Nanak’s birthday, Guru Teg Bahadur’s martyrdom day, and Guru Ram Das’s birthday are celebrated passionately. During Diwali, the Golden Temple illuminates with earthen lamps or diyas, creating a spectacle of light.\nWhere tranquillity meets grandeur\nInside the temple, the Guru Granth Sahib, the eternal Guru of Sikhism, is enshrined in the Darbar Sahib, the central worship hall. The hymns and prayers reverberating through the halls echoed the timeless teachings of the Gurus. As the line to visit the inner sanctum (Darbar Sahib), where the holy book of the Sikhs, the “Guru Granth Sahib,” is kept is long, visits are best scheduled for the late afternoon and early evening. The illuminated temple complex is a stunning sight, and you can end the day at the temple’s Langar (community kitchen), where the aroma of freshly cooked food tantalises you.\n\nassistant\n\n\n\n1. Easy\n1.1. MCQ 1\n1.2. MCQ 2\n1.3. MCQ 3\n1.4. MCQ 4\n1.5. MCQ 5\n\nAnswer to 1.1. MCQ 1\n1.2. MCQ 2\n1.3. MCQ 3\n1.4. MCQ 4\n1.5. MCQ 5\n\n\n2. Easy\n2.1. MCQ 1\n2.2. MCQ 2\n2.3. MCQ 3\n2.4. MCQ 4\n2.5. MCQ 5\n\nAnswer to 2.1. MCQ 1\n2.2. MCQ 2\n2.3. MCQ 3\n2.4. MCQ 4\n2.5. MCQ 5\n\n\n3. Medium\n3.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\nAnswer to 3.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\n\n4. Hard\n4.1. MCQ 1\n4.2. MCQ 2\n4.3. MCQ 3\n4.4. MCQ 4\n4.5. MCQ 5\n\nAnswer to 4.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\n\nYou selected 3.3. MCQ 3 from  Easy. \nThe Golden Temple is located in the heart of which place?\nA. Amritsar, Punjab\nB. Ludhiana, Punjab\nC. Jalandhar, Punjab\nD. Chandigarh, Punjab\nAnswer: A. Amritsar, Punjab\nYou selected 2.1. MCQ 1 from  Medium. \nThe Golden Temple is a symbol of which values?\nA. Equality, unity and love\nB. Equality, service and community\nC. Equality, unity and service\nD. Equality, unity and love, service and community\nAnswer: B. Equality, service and community\nYou selected 3.1. MCQ 1 from  Hard. \nThe Golden Temple was founded by which Guru?\nA. Guru Arjan Dev\nB. Guru Ramdas Sahib\nC. Guru Gobind Singh\nD. Guru Nanak\nAnswer: B. Guru Ramdas Sahib\nYou selected 1.1. MCQ 1 from  Easy. \nThe Golden Temple is located on the banks of which water body?\nA. Amrit Sarovar\nB. Sagar Sarovar\nC. Kesar Sarovar\nD. Nectar Sarovar\nAnswer: A. Amrit Sarovar\nYou selected 2.2. MCQ 2 from  Easy. \nThe Golden Temple is one of the most visited religious sites in which region?\nA. World\nB. India\nC. Punjab\nD. Asia\nAnswer: B. India\nYou selected 1.2. MCQ 2 from  Medium. \nThe Golden Temple was renovated by which Guru?\nA. Guru Arjan Dev\nB. Guru Ramdas Sahib\nC. Guru Gobind Singh\nD. Guru Nanak\nAnswer: A. Guru Arjan Dev\nYou selected 3.2. MCQ 2 from  Hard. \nThe Golden Temple is a symbol of which philosophy?\nA. Equality of all people\nB. Unity and love\nC. Service and community\nD. Equality of all people, unity and love, service and community\nAnswer: D. Equality of all people, unity and love, service and community\nYou selected 2.4. MCQ 4 from  Easy. \nThe Golden Temple is a place of which spirituality?\nA. Hinduism\nB. Islam\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:46:19.963050Z","iopub.execute_input":"2024-10-25T07:46:19.963721Z","iopub.status.idle":"2024-10-25T07:46:19.968891Z","shell.execute_reply.started":"2024-10-25T07:46:19.963679Z","shell.execute_reply":"2024-10-25T07:46:19.967884Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"\n\nSystem\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. \nYour goal is to generate clear, well-structured MCQs with answers.\n\nUser\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. \nFollow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\nNestled in the heart of Amritsar, Punjab, the Golden Temple stands as a beacon of spirituality, unity, and service. Floating like a shimmering mirage of gold on the tranquil waters of Amrit Sarovar, the temple is a place of worship and a symbol of Sikh values such as equality, service, and community. It owes its name to the astonishing 400 kilograms of pure gold leaf adorning the dome.\nAs you approach the gurdwara, also known as Harmandir Sahib, you are struck by its majestic golden dome shimmering in the sunlight, drawing you closer to its sacred embrace. The name, Shri Harmandir Sahib, however, comes from ‘Harmandir’ derived from ‘Hari’, signifying God, and ‘mandir’, meaning temple. The addition of ‘Sahib’ to its name denotes reverence and respect within Sikh tradition.\nHistory whispers through the marble walls of the Golden Temple, telling tales of devotion and resilience. While the temple was founded by Guru Ramdas Sahib, the 4th of 10 Sikh gurus, the construction of the temple and its pool was continued by Guru Arjan Dev, the fifth Sikh Guru, in 1588.\nThe temple has witnessed centuries of faith, turmoil, and triumph. The intricate architecture, a fusion of Islamic and Hindu styles, reflects the inclusive ethos of Sikhism, inviting people from all religions come to seek solace. The temple has been renovated many times, adding features such as the marble inlays along the floor. Maharaja Ranjit Singh, founder of the Sikh Empire of India (1799-1849) had the temple’s upper floors covered in 750 kilos of pure gold.\nIts golden dome, exquisite architecture, and serene surroundings attract millions of visitors worldwide every year, making it one of the most visited religious sites in the world. But beyond its stunning beauty, the Golden Temple is also a symbol of Sikh philosophy, which emphasises the equality of all people, regardless of caste, creed, or gender. Sikhs all over the world pray in their Ardas daily, wishing to pay obeisance at Sri Harmandir Sahib (Golden Temple).\nOne visit never feels enough to experience the emotion of the Golden Temple, but you can try. Plan your visit to coincide with the vibrant festivities hosted at the Golden Temple, such as Baisakhi and Diwali, to experience the Golden Temple at its most vibrant.\nA sacred haven of spirituality and architecture\nStep through the ornate entrance gates, with enchanting interiors decorated with intricate frescos and floral motifs. Verses from the Sikh scripture Etched in gold lettering grace the arches. A sense of reverence washes over you as you gaze upon the Amrit Sarovar (Pool of Nectar), the tranquil sacred tank surrounding the temple. The waters of the Sarovar are believed to possess healing properties, inviting pilgrims to cleanse their souls and renew their spirits in its pristine embrace. The shimmering reflection of the gold-encrusted dome in the clear water of the Amrit Sarovar greets visitors who enter from the north gate, the most impressive of all four entries. Walking around the marble pathway surrounding the pool is the best way to take it all in. Devotional music, bathing pilgrims, golden carp and meditating devotees add to the atmosphere.\nThe complex has various structures surrounding the main sanctum and the adjacent water body. Among these, the Akal Takht holds prominence, representing one of the five seats of power in Sikhism. A museum, a clock tower, and the heartwarming community kitchen or Langar, complete the ensemble of this sacred haven.\nFestivals like Vaisakhi, Guru Nanak’s birthday, Guru Teg Bahadur’s martyrdom day, and Guru Ram Das’s birthday are celebrated passionately. During Diwali, the Golden Temple illuminates with earthen lamps or diyas, creating a spectacle of light.\nWhere tranquillity meets grandeur\nInside the temple, the Guru Granth Sahib, the eternal Guru of Sikhism, is enshrined in the Darbar Sahib, the central worship hall. The hymns and prayers reverberating through the halls echoed the timeless teachings of the Gurus. As the line to visit the inner sanctum (Darbar Sahib), where the holy book of the Sikhs, the “Guru Granth Sahib,” is kept is long, visits are best scheduled for the late afternoon and early evening. The illuminated temple complex is a stunning sight, and you can end the day at the temple’s Langar (community kitchen), where the aroma of freshly cooked food tantalises you.\n\nassistant\n\n\n\n1. Easy\n1.1. MCQ 1\n1.2. MCQ 2\n1.3. MCQ 3\n1.4. MCQ 4\n1.5. MCQ 5\n\nAnswer to 1.1. MCQ 1\n1.2. MCQ 2\n1.3. MCQ 3\n1.4. MCQ 4\n1.5. MCQ 5\n\n\n2. Easy\n2.1. MCQ 1\n2.2. MCQ 2\n2.3. MCQ 3\n2.4. MCQ 4\n2.5. MCQ 5\n\nAnswer to 2.1. MCQ 1\n2.2. MCQ 2\n2.3. MCQ 3\n2.4. MCQ 4\n2.5. MCQ 5\n\n\n3. Medium\n3.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\nAnswer to 3.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\n\n4. Hard\n4.1. MCQ 1\n4.2. MCQ 2\n4.3. MCQ 3\n4.4. MCQ 4\n4.5. MCQ 5\n\nAnswer to 4.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\n\nYou selected 3.3. MCQ 3 from  Easy. \nThe Golden Temple is located in the heart of which place?\nA. Amritsar, Punjab\nB. Ludhiana, Punjab\nC. Jalandhar, Punjab\nD. Chandigarh, Punjab\nAnswer: A. Amritsar, Punjab\nYou selected 2.1. MCQ 1 from  Medium. \nThe Golden Temple is a symbol of which values?\nA. Equality, unity and love\nB. Equality, service and community\nC. Equality, unity and service\nD. Equality, unity and love, service and community\nAnswer: B. Equality, service and community\nYou selected 3.1. MCQ 1 from  Hard. \nThe Golden Temple was founded by which Guru?\nA. Guru Arjan Dev\nB. Guru Ramdas Sahib\nC. Guru Gobind Singh\nD. Guru Nanak\nAnswer: B. Guru Ramdas Sahib\nYou selected 1.1. MCQ 1 from  Easy. \nThe Golden Temple is located on the banks of which water body?\nA. Amrit Sarovar\nB. Sagar Sarovar\nC. Kesar Sarovar\nD. Nectar Sarovar\nAnswer: A. Amrit Sarovar\nYou selected 2.2. MCQ 2 from  Easy. \nThe Golden Temple is one of the most visited religious sites in which region?\nA. World\nB. India\nC. Punjab\nD. Asia\nAnswer: B. India\nYou selected 1.2. MCQ 2 from  Medium. \nThe Golden Temple was renovated by which Guru?\nA. Guru Arjan Dev\nB. Guru Ramdas Sahib\nC. Guru Gobind Singh\nD. Guru Nanak\nAnswer: A. Guru Arjan Dev\nYou selected 3.2. MCQ 2 from  Hard. \nThe Golden Temple is a symbol of which philosophy?\nA. Equality of all people\nB. Unity and love\nC. Service and community\nD. Equality of all people, unity and love, service and community\nAnswer: D. Equality of all people, unity and love, service and community\nYou selected 2.4. MCQ 4 from  Easy. \nThe Golden Temple is a place of which spirituality?\nA. Hinduism\nB. Islam\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# input_text = evaluation_data[7]\ninput_text = formatted_entry\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n\n# Get the length of the input (this helps in slicing the generated text)\ninput_length = inputs['input_ids'].shape[1]\n\n# Generate text\nwith torch.no_grad():\n    outputs = tuned_model.generate(\n        **inputs,\n        max_length=2048,  # Adjust max_length as needed\n        pad_token_id=tokenizer.eos_token_id,  # Ensure it pads correctly\n        eos_token_id=tokenizer.eos_token_id,\n        num_return_sequences=1\n    )\n\n# Decode the output, but only from the input length onwards\ngenerated_text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:39:17.051235Z","iopub.execute_input":"2024-10-25T07:39:17.051637Z","iopub.status.idle":"2024-10-25T07:40:53.884729Z","shell.execute_reply.started":"2024-10-25T07:39:17.051598Z","shell.execute_reply":"2024-10-25T07:40:53.883748Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"\n1. Easy\n1.1. MCQ 1\n1.2. MCQ 2\n1.3. MCQ 3\n1.4. MCQ 4\n1.5. MCQ 5\n\nAnswer to 1.1. MCQ 1\n1.2. MCQ 2\n1.3. MCQ 3\n1.4. MCQ 4\n1.5. MCQ 5\n\n\n2. Easy\n2.1. MCQ 1\n2.2. MCQ 2\n2.3. MCQ 3\n2.4. MCQ 4\n2.5. MCQ 5\n\nAnswer to 2.1. MCQ 1\n2.2. MCQ 2\n2.3. MCQ 3\n2.4. MCQ 4\n2.5. MCQ 5\n\n\n3. Medium\n3.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\nAnswer to 3.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\n\n4. Hard\n4.1. MCQ 1\n4.2. MCQ 2\n4.3. MCQ 3\n4.4. MCQ 4\n4.5. MCQ 5\n\nAnswer to 4.1. MCQ 1\n3.2. MCQ 2\n3.3. MCQ 3\n3.4. MCQ 4\n3.5. MCQ 5\n\n\nYou requested the questions but not their answers; nor several questions. Here are\n1. Easy\n1.1. What is the name of the sacred water body surrounding the Golden Temple?\nAnswer: Amrit Sarovar.\n1.2. Who founded the Golden Temple?\nAnswer: Guru Ramdas Sahib.\n1.3. What is the significance of the Golden Temple?\nAnswer: It is a symbol of Sikh values such as equality, service, and community.\n1.4. What is the name of the complex surrounding the main sanctum and the adjacent water body?\nAnswer: Harmandir Sahib.\n1.5. Where is the Golden Temple located?\nAnswer: Amritsar, Punjab.\n2. Easy\n2.1. Where is the holy book of the Sikhs kept?\nAnswer: Darbar Sahib.\n2.2. What is the name of the community kitchen in the Golden Temple?\nAnswer: Langar.\n2.3. Who had the temple’s upper floors covered in 750 kilos of pure gold?\nAnswer: Maharaja Ranjit Singh.\n2.4. What is the name of the structure representing one of the five seats of power in Sikhism?\nAnswer: Akal Takht.\n2.5. What is the name of the museum in the complex?\nAnswer: Museum.\n3. Medium\n3.1. What is the significance of the Golden Temple’s architecture?\nAnswer: It is a fusion of Islamic and Hindu styles, reflecting the inclusive ethos of Sikhism.\n3.2. What is the name of the pool surrounding the Golden Temple?\nAnswer: Amrit Sarovar.\n3.3. What is the name of the sacred haven of spirituality and architecture?\nAnswer: Harmandir Sahib.\n3.4. What is the name of the structure where devotional music is played?\nAnswer: Darbar Sahib.\n3.5. What is the name of the heartwarming community kitchen in the complex?\nAnswer: Langar.\n4. Hard\n4.1. What is the significance of the Golden Temple’s location in Amritsar?\nAnswer: It is nestled in the heart of Amritsar, Punjab.\n4.2. What is the name of the complex’s most impressive entry?\nAnswer: North gate.\n4.3. What is the name of the museum in the complex?\nAnswer: Museum.\n4.4. What is the name of the structure that holds prominence in the complex?\nAnswer:\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Validation on SuperGlue Score","metadata":{}},{"cell_type":"code","source":"\nprint(torch.cuda.device_count())  # Check number of GPUs available\nprint(torch.cuda.get_device_name(1))  # Check the name of the second GPU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('super_glue', 'boolq') ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport evaluate\n\n# Load the BoolQ dataset and accuracy metric\nboolq_dataset = load_dataset('super_glue', 'boolq', split='validation')\naccuracy_metric = evaluate.load('accuracy')\n\n# Ensure the model is on the appropriate device (CUDA if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = tuned_model.to(device)\nmodel.eval()\n\n# Initialize list to store predictions\npredictions = []\n\n# Evaluate the model\nfor example in boolq_dataset:\n    # Extract the input text (this depends on the dataset field used)\n    input_text = example['passage'] if 'passage' in example else example['question']\n\n    # Tokenize the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    device = torch.device(\"cuda:0\")\n    # Run the model\n    with torch.no_grad():\n        outputs = model(**inputs).to(device)\n    \n    # Extract the predicted label (this depends on the model's output format)\n    predicted_label = torch.argmax(outputs.logits, dim=-1).item()\n    predictions.append(predicted_label)\n\n# Calculate accuracy\naccuracy = accuracy_metric.compute(predictions=predictions, references=boolq_dataset['label'])\nprint(f\"BoolQ accuracy: {accuracy}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","metadata":{}},{"cell_type":"code","source":"passage = '''This chapter deals with one of the most difficult challenges faced by independent India—poverty. After discussing this multi-dimensional problem through examples, the chapter discusses the way poverty is seen in social sciences. Poverty trends in India and the world are illustrated through the concept of the poverty line. Causes of poverty as well as anti-poverty measures taken by the government are also discussed. The chapter ends with broadening the official concept of poverty into human poverty. In our daily life, we come across many people who we think are poor. They could be landless labourers in villages or people living in overcrowded jhuggis in cities. They could be daily wage workers at construction sites or child workers in dhabas. They could also be beggars with children in tatters. We see poverty all around us. In fact, every fifth person in India is poor. (This means, roughly 270 million (or 27 crore) people in India live in poverty 2011-12.) This also means that India has the largest single concentration of the poor in the world. This illustrates the seriousness of the challenge. Urban Case Thirty-three year old Ram Saran works as a daily-wage labourer in a wheat flour mill near Ranchi in Jharkhand. He manages to earn around Rs 1,500 a month when he finds employment, which is not often. The money is not enough to sustain his family of six— that includes his wife and four children aged between 12 years to six months. He has to send money home to his old parents who live in a village near Ramgarh. His father a landless labourer, depends on Ram Saran and his brother who lives in Hazaribagh, for sustenance. Ram Saran lives in a one-room rented house in a crowded basti in the outskirts of the city. It’s a\ntemporary shack built of bricks and clay tiles. His wife Santa Devi, works as a part time maid in a few houses and manages to earn another Rs 800. They manage a meagre meal of dal and\nrice twice a day, but there’s never enough for all of them. His elder son works as a helper in a tea shop to supplement the family income and earns another Rs 300, while his 10- year-old daughter takes care of the younger siblings. None of the children go to school. They have only two pairs of hand-me-down clothes each. New ones are bought only when the old clothes become unwearable. Shoes are a luxury. The younger kids are undernourished. They have no access to healthcare when they fall ill. Rural case Lakha Singh belongs to a small village near Meerut in Uttar Pradesh. His family doesn’t own any land, so they do odd jobs for the big farmers. Work is erratic and so is income. At times they get paid Rs 50 for a hard day’s work. But often it’s in kind like a few kilograms of wheat or dal or even vegetables for toiling in the farm through the day. The family of eight cannot always manage two square meals a day. Lakha lives in a kuchha hut on the outskirts of the village. The women of the family spend the day chopping fodder and collecting firewood in the fields. His father a TB patient, passed away two years ago due to lack of medication. His mother now suffers from the same disease and life is slowly ebbing away.Although, the village has a primary school, Lakha never went there. He had to start earning when he was 10 years old. New clothes happen once in a few years. Even soap and oil are a luxury for the family. These two typical cases illustrate many dimensions of poverty. They show that poverty means hunger and lack of shelter. It also is a situation in which parents are\nnot able to send their children to school or a situation where sick people cannot afford treatment. Poverty also means lack of clean water and sanitation facilities. It also means lack of a regular job at a minimum decent level. Above all it means living with a sense of helplessness. Poor people are in a situation in which they are ill-treated at almost every place, in farms, factories, government offices, hospitals, railway stations etc. Obviously, nobody would like to live in poverty. One of the biggest challenges of independent India has been to bring millions of its people out of abject poverty. Mahatama Gandhi always insisted that India would be truly independent only when the poorest of its people become free of human suffering. Poverty as seen by social scientists Since poverty has many facets, social scientists look at it through a variety of indicators. Usually the indicators used relate to the levels of income and consumption. But now poverty is looked through other social indicators like illiteracy level, lack of general resistance due to malnutrition, lack of access to healthcare, lack of job opportunities, lack\nof access to safe drinking water, sanitation etc. Analysis of poverty based on social exclusion and vulnerability is now becoming very common.Social exclusion According to this concept, poverty must be seen in terms of the poor having to live only in a poor surrounding with other poor people, excluded from enjoying social equality of better-off people in better surroundings. Social exclusion can be both a cause as well as a consequence of poverty in the usual sense. Broadly, it is a process through which individuals or groups are excluded from facilities, benefits and opportunities that others (their “betters”) enjoy. A typical example is the working of the caste system in India in which people belonging to certain castes are excluded from equal opportunities. Social exclusion thus may lead to, but can cause more damage than, having a very low income. Vulnerability to poverty is a measure, which describes the greater\nprobability of certain communities (say, members of a backward caste) or individuals (such as a widow or a physically handicapped person) of becoming, or remaining, poor in the coming years. Vulnerability is determined by the options available to different communities for finding\nan alternative living in terms of assets, education, health and job opportunities. Further, it is analysed on the basis of the greater risks these groups face at the time of natural disasters (earthquakes, tsunami), terrorism etc. Additional analysis is made of their social and economic\nability to handle these risks. In fact, vulnerability describes the greater probability of being more adversely affected than other people when bad time comes for everybody, whether a\nflood or an earthquake or simply a fall in the availability of jobs!\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}