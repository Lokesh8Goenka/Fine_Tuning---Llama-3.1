{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9401942,"sourceType":"datasetVersion","datasetId":5707438},{"sourceId":9655730,"sourceType":"datasetVersion","datasetId":5898444},{"sourceId":104449,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102},{"sourceId":124809,"sourceType":"modelInstanceVersion","modelInstanceId":105041,"modelId":129263},{"sourceId":141778,"sourceType":"modelInstanceVersion","modelInstanceId":120095,"modelId":143319},{"sourceId":145314,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":123196,"modelId":146251}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ******************************* 0 ******************************* \n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n!pip install jinja2\n!pip install -U bitsandbytes\n!pip install peft \n!pip install accelerate --upgrade\n!pip install --upgrade transformers\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport bitsandbytes as bnb\nfrom transformers import LlamaForCausalLM, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig\nfrom transformers import AutoTokenizer\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T09:03:26.435029Z","iopub.execute_input":"2024-10-24T09:03:26.435312Z","iopub.status.idle":"2024-10-24T09:05:10.294370Z","shell.execute_reply.started":"2024-10-24T09:03:26.435279Z","shell.execute_reply":"2024-10-24T09:05:10.293461Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2) (2.1.5)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nCollecting accelerate\n  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.33.0\n    Uninstalling accelerate-0.33.0:\n      Successfully uninstalled accelerate-0.33.0\nSuccessfully installed accelerate-1.0.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed tokenizers-0.20.1 transformers-4.46.0\n/kaggle/input/prefix_model_25/transformers/v1/1/adapter_model.safetensors\n/kaggle/input/prefix_model_25/transformers/v1/1/training_args.bin\n/kaggle/input/prefix_model_25/transformers/v1/1/adapter_config.json\n/kaggle/input/prefix_model_25/transformers/v1/1/README.md\n/kaggle/input/prefix_model_25/transformers/v1/1/tokenizer.json\n/kaggle/input/prefix_model_25/transformers/v1/1/tokenizer_config.json\n/kaggle/input/prefix_model_25/transformers/v1/1/special_tokens_map.json\n/kaggle/input/prefix_model_25/transformers/v1/1/lora/adapter_model.safetensors\n/kaggle/input/prefix_model_25/transformers/v1/1/lora/adapter_config.json\n/kaggle/input/prefix_model_25/transformers/v1/1/lora/README.md\n/kaggle/input/qlora_model_20/transformers/v1/1/adapter_model.safetensors\n/kaggle/input/qlora_model_20/transformers/v1/1/training_args.bin\n/kaggle/input/qlora_model_20/transformers/v1/1/adapter_config.json\n/kaggle/input/qlora_model_20/transformers/v1/1/README.md\n/kaggle/input/qlora_model_20/transformers/v1/1/tokenizer.json\n/kaggle/input/qlora_model_20/transformers/v1/1/tokenizer_config.json\n/kaggle/input/qlora_model_20/transformers/v1/1/special_tokens_map.json\n/kaggle/input/qlora_model_20/transformers/v1/1/lora/adapter_model.safetensors\n/kaggle/input/qlora_model_20/transformers/v1/1/lora/adapter_config.json\n/kaggle/input/qlora_model_20/transformers/v1/1/lora/README.md\n/kaggle/input/combined-data/combines_dataset.csv\n/kaggle/input/rephrased-data/combined_dataset.csv\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/adapter_model.safetensors\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/config.json\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/training_args.bin\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/adapter_config.json\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/README.md\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/tokenizer.json\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/tokenizer_config.json\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/special_tokens_map.json\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/lora/adapter_model.safetensors\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/lora/adapter_config.json\n/kaggle/input/llama3.1_15_epoch/transformers/v1/2/lora/README.md\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Necessary until transformers package is updated in the Kaggle notebook environment.\n\n\n# import transformers\n# import torch\n\n# torch.backends.cuda.enable_mem_efficient_sdp(False)\n# torch.backends.cuda.enable_flash_sdp(False)\n\n# model = \"/kaggle/input/llama-3.1/transformers/8b/1\"","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:19:45.100331Z","iopub.execute_input":"2024-10-22T06:19:45.100659Z","iopub.status.idle":"2024-10-22T06:19:45.105666Z","shell.execute_reply.started":"2024-10-22T06:19:45.100626Z","shell.execute_reply":"2024-10-22T06:19:45.104673Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Llama 3.1 8B-Instruct","metadata":{}},{"cell_type":"code","source":"# prompt = '''Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\n# The Test consists of three different sections where each section contains 5 questions each of easy, medium and hard difficulty level.\n# Use Bloom's Taxonomy to define the difficulty level of the question.\n# Remember the questions should be from the passage only. \n# '''\n# passage = '''Horticulture Growing vegetables, flowers and fruits for commercial use.\n# Farm System Agriculture or farming can be looked at as a system. The important inputs are\n# seeds, fertilisers, machinery and labour. Some of the operations involved are ploughing, sowing,\n# irrigation, weeding and harvesting. The outputs from the system include crops, wool, dairy and\n# poultry products. Types of Farming Farming is practised in various ways across the world.\n# Depending upon the geographical conditions, demand of produce, labour and level of\n# technology, farming can be classified into two main types. These are subsistence farming and\n# commercial farming. Subsistence Farming This type of farming is practised to meet the needs of\n# the farmer’s family. Traditionally, low levels of technology and household labour are used to\n# produce on small output. Subsistence farming can be further classified as intensive subsistence\n# and primitive subsistence farming. In intensive subsistence agriculture the farmer cultivates a\n# small plot of land using simple tools and more labour. Climate with large number of days with\n# sunshine and fertile soils permit growing of more than one crop annually on the same plot. Rice\n# is the main crop. Other crops include wheat, maize, pulses and oilseeds. Intensive subsistence\n# agriculture is prevalent in the thickly populated areas of the monsoon regions of south,\n# southeast and east Asia. Primitive subsistence agriculture includes shifting cultivation and\n# nomadic herding. Shifting cultivation is practised in the thickly forested areas of Amazon basin,\n# tropical Africa, parts of southeast Asia and Northeast India. These are the areas of heavy\n# rainfall and quick regeneration of vegetation. A plot of land is cleared by felling the trees and\n# burning them. The ashes are then mixed with the soil and crops like maize, yam, potatoes and\n# cassava are grown. After the soil loses its fertility, the land is abandoned and the cultivator\n# moves to a new plot. Shifting cultivation is also known as ‘slash and burn’ agriculture. Nomadic\n# herding is practised in the semi-arid and arid regions of Sahara, Central Asia and some parts of\n# India, like Rajasthan and Jammu and Kashmir. In this type of farming, herdsmen move from\n# place to place with their animals for fodder and water, along defined routes. This type of\n# movement arises in response to climatic constraints and terrain. Sheep, camel, yak and goatsare most commonly reared. They provide milk, meat, wool, hides and other products to the\n# herders and their families.'''","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:19:45.106967Z","iopub.execute_input":"2024-10-22T06:19:45.107310Z","iopub.status.idle":"2024-10-22T06:19:45.120190Z","shell.execute_reply.started":"2024-10-22T06:19:45.107275Z","shell.execute_reply":"2024-10-22T06:19:45.119188Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# # Necessary until transformers packages is updated in the Kaggle notebook environment.\n# !pip install --upgrade transformers\n\n# import transformers\n# import torch\n\n# torch.backends.cuda.enable_mem_efficient_sdp(False)\n# torch.backends.cuda.enable_flash_sdp(False)\n\n# model_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\n# pipeline = transformers.pipeline(\n#     \"text-generation\",\n#     model=model_id,\n#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n#     device_map=\"auto\",\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:19:45.121358Z","iopub.execute_input":"2024-10-22T06:19:45.121646Z","iopub.status.idle":"2024-10-22T06:19:45.134645Z","shell.execute_reply.started":"2024-10-22T06:19:45.121614Z","shell.execute_reply":"2024-10-22T06:19:45.133705Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# prompt = '''Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\n# The Test consists of three different sections where each section contains 5 questions each of easy, medium and hard difficulty level.\n# Use Bloom's Taxonomy to define the difficulty level of the question.\n# Remember the questions should be from the passage only. '''","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:19:45.145357Z","iopub.execute_input":"2024-10-22T06:19:45.145658Z","iopub.status.idle":"2024-10-22T06:19:45.149670Z","shell.execute_reply.started":"2024-10-22T06:19:45.145625Z","shell.execute_reply":"2024-10-22T06:19:45.148783Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\n# messages = [\n#     {\"role\": \"system\", \"content\": \"You are an assitant social studies teacher in a High School.\"},\n#     {\"role\": \"user\", \"content\": prompt + passage},\n# ]\n\n# outputs = pipeline(\n#     messages,\n#     max_new_tokens=2048,\n# )\n# print(outputs[0][\"generated_text\"][-1])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:19:45.190670Z","iopub.execute_input":"2024-10-22T06:19:45.190979Z","iopub.status.idle":"2024-10-22T06:19:45.195429Z","shell.execute_reply.started":"2024-10-22T06:19:45.190944Z","shell.execute_reply":"2024-10-22T06:19:45.194462Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### Generated MCQS on ","metadata":{}},{"cell_type":"markdown","source":"\"**Section 1: Recall (Easy Difficulty Level)**\\n\\n1. What is Horticulture?\\n   a) Growing vegetables, flowers and fruits for commercial use\\n   b) Farming for subsistence\\n   c) Farming for commercial use\\n   d) Growing crops for household use\\n\\nAnswer: a) Growing vegetables, flowers and fruits for commercial use\\n\\n2. What are the important inputs in a farm system?\\n   a) Seeds, fertilisers, machinery and labour\\n   b) Seeds, fertilisers, animals and soil\\n   c) Seeds, labour, machinery and water\\n   d) Seeds, fertilisers, animals and tools\\n\\nAnswer: a) Seeds, fertilisers, machinery and labour\\n\\n3. What are the outputs from a farm system?\\n   a) Crops, wool, dairy and poultry products\\n   b) Crops, seeds, fertilisers and labour\\n   c) Crops, animals, tools and water\\n   d) Crops, animals, seeds and soil\\n\\nAnswer: a) Crops, wool, dairy and poultry products\\n\\n4. What is subsistence farming?\\n   a) Farming to meet the needs of the farmer's family\\n   b) Farming for commercial use\\n   c) Farming for household use\\n   d) Farming for export\\n\\nAnswer: a) Farming to meet the needs of the farmer's family\\n\\n5. What is intensive subsistence agriculture?\\n   a) Farming using simple tools and more labour\\n   b) Farming using complex tools and less labour\\n   c) Farming using animals and household labour\\n   d) Farming using machines and commercial labour\\n\\nAnswer: a) Farming using simple tools and more labour\\n\\n\n**Section 2: Analyze (Medium Difficulty Level)**\\n\\n6. What are the geographical conditions that permit intensive subsistence agriculture? (Bloom's Taxonomy: Analyze)\\n   a) Large number of days with sunshine and fertile soils\\n   b) Heavy rainfall and quick regeneration of vegetation\\n   c) Semi-arid and arid regions\\n   d) Thickly populated areas\\n\\nAnswer: a) Large number of days with sunshine and fertile soils\\n\\n7. What are the main crops grown in intensive subsistence agriculture? (Bloom's Taxonomy: Analyze)\\n   a) Rice, wheat, maize, pulses and oilseeds\\n   b) Maize, yam, potatoes and cassava\\n   c) Sheep, camel, yak and goats\\n   d) Cattle, buffalo and horses\\n\\nAnswer: a) Rice, wheat, maize, pulses and oilseeds\\n\\n8. What is shifting cultivation? (Bloom's Taxonomy: Analyze)\\n   a) Farming using simple tools and more labour\\n   b) Farming using animals and household labour\\n   c) Shifting from one plot of land to another\\n   d) Farming using machines and commercial labour\\n\\nAnswer: c) Shifting from one plot of land to another\\n\\n9. What is nomadic herding? (Bloom's Taxonomy: Analyze)\\n   a) Farming using simple tools and more labour\\n   b) Farming using animals and household labour\\n   c) Moving from place to place with animals for fodder and water\\n   d) Farming using machines and commercial labour\\n\\nAnswer: c) Moving from place to place with animals for fodder and water\\n\\n10. What are the products provided by sheep, camel, yak and goats in nomadic herding? (Bloom's Taxonomy: Analyze)\\n    a) Milk, meat, wool, hides and other products\\n    b) Crops, seeds, fertilisers and labour\\n    c) Crops, animals, tools and water\\n    d) Crops, animals, seeds and soil\\n\\nAnswer: a) Milk, meat, wool, hides and other products\\n\\n\n**Section 3: Evaluate (Hard Difficulty Level)**\\n\\n11. What are the limitations of subsistence farming? (Bloom's Taxonomy: Evaluate)\\n    a) Low levels of technology and household labour\\n    b) High levels of technology and commercial labour\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: a) Low levels of technology and household labour\\n\\n12. What are the advantages of intensive subsistence agriculture? (Bloom's Taxonomy: Evaluate)\\n    a) High levels of technology and commercial labour\\n    b) Low levels of technology and household labour\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: c) Large number of days with sunshine and fertile soils\\n\\n13. What are the limitations of shifting cultivation? (Bloom's Taxonomy: Evaluate)\\n    a) Soil loses its fertility and is abandoned\\n    b) Soil retains its fertility and is reused\\n    c) Large number of days with sunshine and fertile soils\\n    d) Heavy rainfall and quick regeneration of vegetation\\n\\nAnswer: a) Soil loses its fertility and is abandoned\\n\\n14. What are the advantages of nomadic herding? (Bloom's Taxonomy: Evaluate)\\n    a) High levels of technology and commercial labour\\n    b) Low levels of technology and household labour\\n    c) Moving from place to place with animals for fodder and water\\n    d) Farming using machines and commercial labour\\n\\nAnswer: c) Moving from place to place with animals for fodder and water\\n\\n15. What are the geographical conditions that permit nomadic herding? (Bloom's Taxonomy: Evaluate)\\n    a) Large number of days with sunshine and fertile soils\\n    b) Heavy rainfall and quick regeneration of vegetation\\n    c) Semi-arid and arid regions\\n    d) Thickly populated areas\\n\\nAnswer: c) Semi-arid and arid regions\"}","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"## Using ADALORA\n","metadata":{}},{"cell_type":"code","source":"model_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:05:10.296308Z","iopub.execute_input":"2024-10-24T09:05:10.296923Z","iopub.status.idle":"2024-10-24T09:05:10.301161Z","shell.execute_reply.started":"2024-10-24T09:05:10.296882Z","shell.execute_reply":"2024-10-24T09:05:10.300179Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# loading dataset","metadata":{}},{"cell_type":"code","source":"# ******************************* 1 ******************************* \n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/combined-data/combines_dataset.csv', encoding='ISO-8859-1')\n# df = pd.read_csv('/kaggle/input/rephrased-data/combined_dataset.csv', encoding='ISO-8859-1')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:05:10.302430Z","iopub.execute_input":"2024-10-24T09:05:10.302827Z","iopub.status.idle":"2024-10-24T09:05:10.384490Z","shell.execute_reply.started":"2024-10-24T09:05:10.302778Z","shell.execute_reply":"2024-10-24T09:05:10.383571Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndf_old = df\n\n# Drop empty rows\ndf = df_old.dropna(how='all')  # This removes rows where all elements are NaN (empty)\n\n# Print the number of rows after cleaning\nprint(f\"Number of rows after cleaning: {len(df)}\")\n\n# Optionally, save the cleaned dataset\n# df.to_csv('cleaned_dataset.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:05:10.386272Z","iopub.execute_input":"2024-10-24T09:05:10.386860Z","iopub.status.idle":"2024-10-24T09:05:10.470290Z","shell.execute_reply.started":"2024-10-24T09:05:10.386826Z","shell.execute_reply":"2024-10-24T09:05:10.469449Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of rows after cleaning: 143\n","output_type":"stream"}]},{"cell_type":"code","source":"# ******************************* 1.1 ******************************* \n# Display the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:05:10.471431Z","iopub.execute_input":"2024-10-24T09:05:10.471830Z","iopub.status.idle":"2024-10-24T09:05:10.490502Z","shell.execute_reply.started":"2024-10-24T09:05:10.471786Z","shell.execute_reply":"2024-10-24T09:05:10.489696Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                             passage  \\\n0  This transformation from a plant to a finished...   \n1  People are a nationâs greatest resource. Nat...   \n2  Have you ever given a thought to the fact that...   \n3  In a small village in Tanzania, Africa, Mamba ...   \n4  Water, electricity, rickshaw, vegetable and te...   \n\n                                                mcqs  \n0  ['## Agriculture and Economic Activities: An M...  \n1  ['## Population Studies Test\\n', '\\n', '**Inst...  \n2  ['## The Journey of Your Notebook: A Social St...  \n3  ['## Social Studies Test: Land as a Resource\\n...  \n4  ['## Agriculture and Farming: An MCQ Test\\n', ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>passage</th>\n      <th>mcqs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>This transformation from a plant to a finished...</td>\n      <td>['## Agriculture and Economic Activities: An M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>People are a nationâs greatest resource. Nat...</td>\n      <td>['## Population Studies Test\\n', '\\n', '**Inst...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Have you ever given a thought to the fact that...</td>\n      <td>['## The Journey of Your Notebook: A Social St...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In a small village in Tanzania, Africa, Mamba ...</td>\n      <td>['## Social Studies Test: Land as a Resource\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Water, electricity, rickshaw, vegetable and te...</td>\n      <td>['## Agriculture and Farming: An MCQ Test\\n', ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# ******************************* 1.3 ******************************* \n# Import the splitting function\nfrom sklearn.model_selection import train_test_split\n\n# Split the formatted data into training and testing sets\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Check the lengths of the train and test splits\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Testing set size: {len(test_data)}\")\n\n# Optional: Print the first entry from the training and test set\nprint(f\"First training example: {train_data.iloc[0]}\")\nprint(len(train_data),\"\\n\")\nprint(f\"First testing example: {test_data.iloc[0]}\")\nprint(len(test_data))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:05:29.187439Z","iopub.execute_input":"2024-10-24T09:05:29.187869Z","iopub.status.idle":"2024-10-24T09:05:29.209845Z","shell.execute_reply.started":"2024-10-24T09:05:29.187827Z","shell.execute_reply":"2024-10-24T09:05:29.208985Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training set size: 114\nTesting set size: 29\nFirst training example: passage    1. Peasants and Agricultural Production The ba...\nmcqs       ['## Peasant Life and Agriculture in Mughal In...\nName: 124, dtype: object\n114 \n\nFirst testing example: passage    1. A Mosaic of Religious Beliefs and Practices...\nmcqs       ['## MCQ Test: A Mosaic of Religious Beliefs\\n...\nName: 117, dtype: object\n29\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T14:53:28.081294Z","iopub.execute_input":"2024-10-23T14:53:28.082167Z","iopub.status.idle":"2024-10-23T14:53:28.086763Z","shell.execute_reply.started":"2024-10-23T14:53:28.082125Z","shell.execute_reply":"2024-10-23T14:53:28.085825Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"passages = train_data['passage'].tolist()\nmcqs = train_data['mcqs'].tolist()\n\npassage_test = test_data['passage'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:05:48.366885Z","iopub.execute_input":"2024-10-24T09:05:48.368121Z","iopub.status.idle":"2024-10-24T09:05:48.373217Z","shell.execute_reply.started":"2024-10-24T09:05:48.368070Z","shell.execute_reply":"2024-10-24T09:05:48.372100Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## New Prompt\n<|begin_of_text|>\n<|start_header_id|>MCQs_Generator<|end_header_id|>\nYou are a helpful MCQ Generator and Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\nThe Test consists of three different sections where each section contains 5 questions each of easy, medium and hard difficulty level.\nUse Bloom's Taxonomy to define the difficulty level of the question.\nRemember the questions should be from the passage only. <|eot_id|>\n\n<|start_header_id|>User<|end_header_id|>\npassage<|eot_id|>\n\n<|start_header_id|>MCQs_Generator<|end_header_id|>\nmcqs<|eot_id|>\n\n<|end_of_text|>\n","metadata":{}},{"cell_type":"code","source":"# ******************************* 1.2 ******************************* \ntrain_data = []\n\nfor passage, mcq in zip(passages, mcqs):\n    formatted_entry = f'''\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>User<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage: \n{passage} <|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n{mcq}<|eot_id|>\n<|end_of_text|>\n'''\n    train_data.append(formatted_entry)\n\n# print(train_data[72])","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:23:20.042865Z","iopub.execute_input":"2024-10-24T10:23:20.043848Z","iopub.status.idle":"2024-10-24T10:23:20.050390Z","shell.execute_reply.started":"2024-10-24T10:23:20.043792Z","shell.execute_reply":"2024-10-24T10:23:20.049424Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# ******************************* 1.3 ******************************* \nevaluation_data = []\n\nfor passage in (passage_test):\n    formatted_entry =  f'''\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>User<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n{passage} <|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n<|eot_id|>\n<|end_of_text|>\n'''\n\n\n    evaluation_data.append(formatted_entry)\n\n# print(evaluation_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:23:20.868941Z","iopub.execute_input":"2024-10-24T10:23:20.869590Z","iopub.status.idle":"2024-10-24T10:23:20.875706Z","shell.execute_reply.started":"2024-10-24T10:23:20.869547Z","shell.execute_reply":"2024-10-24T10:23:20.874613Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\n# Define a BitsAndBytesConfig for 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",  # You can choose \"fp4\" or \"nf4\"\n    bnb_4bit_use_double_quant=True,  # Optional: Use double quantization for better precision\n    bnb_4bit_compute_dtype=\"float16\"  # Set compute to float16 to save memory\n)\n\n# Load the model with the new quantization config\nmodel = LlamaForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,  # Use BitsAndBytesConfig for quantization\n    device_map='auto'  # Automatically map the model to available devices (e.g., GPU)\n)\n\n# Apply LoRA (Low-Rank Adaptation)\npeft_params = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n# ********** OLD *******************\n# lora_config = LoraConfig(\n#     r=4,  \n#     lora_alpha=16, \n#     lora_dropout=0.1, \n#     target_modules=[\"q_proj\", \"v_proj\"] \n# )\n\n# Wrap the model with QLoRA (Quantized Low-Rank Adaptation)\nmodel = get_peft_model(model, peft_params)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:05:52.138030Z","iopub.execute_input":"2024-10-24T09:05:52.138414Z","iopub.status.idle":"2024-10-24T09:07:09.105322Z","shell.execute_reply.started":"2024-10-24T09:05:52.138375Z","shell.execute_reply":"2024-10-24T09:07:09.104329Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fdb13554a904e6dbb855752e7d95592"}},"metadata":{}}]},{"cell_type":"code","source":"# ******************************* 2.1 ******************************* \ntokenizer_base = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-3.1/transformers/8b-instruct/2/\")\ntokenizer_base.add_special_tokens({\n    'pad_token': '[PAD]',\n    'additional_special_tokens': ['<|begin_of_text|>', '<|start_header_id|>', '<|end_header_id|>', '<|eom_id|>', '<|eot_id|>', '<|end_of_text|>']\n})\n\n# Tokenize the formatted data\ntokenized_train_data = tokenizer_base(train_data, truncation=True, padding=\"max_length\", max_length=512)\ntokenized_eval_data = tokenizer_base(evaluation_data, truncation=True, padding=\"max_length\", max_length=512)\n\n# ******************************* 3 *******************************\n# Function to format tokenized data for the Trainer\ndef format_data_for_trainer(tokenized_data):\n    formatted_data = []\n    for i in range(len(tokenized_data['input_ids'])):\n        formatted_data.append({\n            'input_ids': tokenized_data['input_ids'][i],\n            'attention_mask': tokenized_data['attention_mask'][i],  # Include attention mask\n            'labels': tokenized_data['input_ids'][i]  # You can modify labels if needed\n        })\n    return formatted_data\n\nformatted_train_data = format_data_for_trainer(tokenized_train_data)\nformatted_eval_data = format_data_for_trainer(tokenized_eval_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:23:28.431275Z","iopub.execute_input":"2024-10-24T10:23:28.432235Z","iopub.status.idle":"2024-10-24T10:23:29.361506Z","shell.execute_reply.started":"2024-10-24T10:23:28.432193Z","shell.execute_reply":"2024-10-24T10:23:29.360632Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"print(len(tokenizer_base))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:23:30.218165Z","iopub.execute_input":"2024-10-24T10:23:30.218967Z","iopub.status.idle":"2024-10-24T10:23:30.245924Z","shell.execute_reply.started":"2024-10-24T10:23:30.218923Z","shell.execute_reply":"2024-10-24T10:23:30.244915Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"128257\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer_base.vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:23:30.398377Z","iopub.execute_input":"2024-10-24T10:23:30.399155Z","iopub.status.idle":"2024-10-24T10:23:30.403705Z","shell.execute_reply.started":"2024-10-24T10:23:30.399117Z","shell.execute_reply":"2024-10-24T10:23:30.402835Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"128000\n","output_type":"stream"}]},{"cell_type":"code","source":"# ******************************* 3.1 ******************************* \nimport torch\nprint(torch.cuda.device_count())  \ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:23:32.582177Z","iopub.execute_input":"2024-10-24T10:23:32.582553Z","iopub.status.idle":"2024-10-24T10:23:32.693590Z","shell.execute_reply.started":"2024-10-24T10:23:32.582516Z","shell.execute_reply":"2024-10-24T10:23:32.692588Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer as SFTTrainer\n\ntraining_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=25,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    save_steps=500,\n    logging_steps=500,\n    learning_rate=5e-5,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\"\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer_base, model=model)\n\ntrainer = Trainer(\n    model=model,\n    args=training_params,\n    train_dataset=formatted_train_data,\n    eval_dataset=formatted_eval_data,\n    data_collator=data_collator\n    )\n\n\n# trainer = SFTTrainer(\n#     model=model,\n#     train_dataset=formatted_train_data,\n# #     peft_config=peft_params,\n#     dataset_text_field=\"text\",\n#     max_seq_length=None,\n#     tokenizer=tokenizer_base,\n#     args=training_params,\n#     packing=False,\n# )\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:23:33.459470Z","iopub.execute_input":"2024-10-24T10:23:33.460177Z","iopub.status.idle":"2024-10-24T11:35:21.805735Z","shell.execute_reply.started":"2024-10-24T10:23:33.460139Z","shell.execute_reply":"2024-10-24T11:35:21.804859Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [980/980 1:11:43, Epoch 34/35]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.637300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=980, training_loss=0.4461531502859933, metrics={'train_runtime': 4307.9333, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.227, 'total_flos': 9.070441138225152e+16, 'train_loss': 0.4461531502859933, 'epoch': 34.3859649122807})"},"metadata":{}}]},{"cell_type":"code","source":"# # ******************************* 4 ******************************* \n# # Define training arguments for multi-GPU and FP16\n# training_args = TrainingArguments(\n#     output_dir='./results_1',\n#     num_train_epochs=20,\n#     per_device_train_batch_size=2,\n#     per_device_eval_batch_size=2,\n#     save_steps=500,\n#     save_total_limit=1,\n#     logging_dir='./logs',\n#     report_to='none',          # optional: to avoid logging issues with multiple GPUs\n#     ddp_find_unused_parameters=True,  # required for multi-GPU\n#     fp16=True,\n# )\n\n# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer_base, model=model)\n\n# # Initialize the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=formatted_train_data,\n#     eval_dataset=formatted_eval_data,\n#     data_collator=data_collator\n#     )\n\n# # Fine-tune the model\n# trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainer.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the fine-tuned Model","metadata":{}},{"cell_type":"code","source":"# Save the model\ntrainer.save_model(\"./New_QLoRA_model\")  # Saves the model (LoRa weights will be part of it)\n\n# Save the tokenizer\ntokenizer_base.save_pretrained(\"./New_QLoRA_model\")\n\n# Save the LoRa adapter (LoRa-specific weights)\nmodel.save_pretrained(\"./New_QLoRA_model/lora\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:11:38.200030Z","iopub.execute_input":"2024-10-24T10:11:38.200820Z","iopub.status.idle":"2024-10-24T10:11:39.238430Z","shell.execute_reply.started":"2024-10-24T10:11:38.200777Z","shell.execute_reply":"2024-10-24T10:11:39.237161Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Path to your model directory\nmodel_dir = '/kaggle/working/New_QLoRA_model'\n\n# Output zip file path\nzip_filename = '/kaggle/working/New_QLoRA_model.zip'\n\n# Zipping the directory\nshutil.make_archive(zip_filename.replace('.zip', ''), 'zip', model_dir)\n\nprint(\"Model zipped successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T10:11:39.240262Z","iopub.execute_input":"2024-10-24T10:11:39.241171Z","iopub.status.idle":"2024-10-24T10:11:52.479633Z","shell.execute_reply.started":"2024-10-24T10:11:39.241125Z","shell.execute_reply":"2024-10-24T10:11:52.478433Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Model zipped successfully!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing the model","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:50:41.052924Z","iopub.execute_input":"2024-10-24T11:50:41.053319Z","iopub.status.idle":"2024-10-24T11:50:53.950687Z","shell.execute_reply.started":"2024-10-24T11:50:41.053281Z","shell.execute_reply":"2024-10-24T11:50:53.949584Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.0.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:50:53.952827Z","iopub.execute_input":"2024-10-24T11:50:53.953188Z","iopub.status.idle":"2024-10-24T11:51:06.777583Z","shell.execute_reply.started":"2024-10-24T11:50:53.953147Z","shell.execute_reply":"2024-10-24T11:51:06.776446Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:51:06.779136Z","iopub.execute_input":"2024-10-24T11:51:06.779551Z","iopub.status.idle":"2024-10-24T11:51:19.614848Z","shell.execute_reply.started":"2024-10-24T11:51:06.779502Z","shell.execute_reply":"2024-10-24T11:51:19.613729Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import LlamaForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom peft import PeftModel\n\n# Define a BitsAndBytesConfig for 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",  # Choose \"nf4\" for better precision\n    bnb_4bit_use_double_quant=True,  # Optional: Use double quantization\n    bnb_4bit_compute_dtype=\"float16\"  # Set compute to float16 to save memory\n)\n\n# Load the fine-tuned model from Hugging Face\n# finetuned_model_path = \"/kaggle/input/prefix_model_25/transformers/v1/1\"  \nfinetuned_model_path = \"/kaggle/input/qlora-model/transformers/v1/1\"  \n# finetuned_model_path = \"/kaggle/input/40_epochqlora/transformers/v1/1\" \n\n# Load the tokenizer used for fine-tuning\ntokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\nprint(\"0\")\n# Load the model with quantization config\nmodel = LlamaForCausalLM.from_pretrained(\n    finetuned_model_path,\n    ignore_mismatched_sizes=True,\n    quantization_config=quantization_config,  \n    device_map=\"auto\" \n)\nprint(\"1\")\n\n# Now load the LoRA adapter (if needed)\nlora_adapter_path = \"/kaggle/input/qlora-model/transformers/v1/1/lora\"\n# lora_adapter_path = \"/kaggle/input/prefix_model_25/transformers/v1/1/lora\"\n# lora_adapter_path = \"/kaggle/input/40_epochqlora/transformers/v1/1/lora\"\ntuned_model = PeftModel.from_pretrained(model, lora_adapter_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:51:19.617042Z","iopub.execute_input":"2024-10-24T11:51:19.617394Z","iopub.status.idle":"2024-10-24T11:51:41.604758Z","shell.execute_reply.started":"2024-10-24T11:51:19.617355Z","shell.execute_reply":"2024-10-24T11:51:41.603734Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97adcf528dad4b3bb3be97e7a2175a30"}},"metadata":{}},{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"passage = '''Nestled in the heart of Amritsar, Punjab, the Golden Temple stands as a beacon of spirituality, unity, and service. Floating like a shimmering mirage of gold on the tranquil waters of Amrit Sarovar, the temple is a place of worship and a symbol of Sikh values such as equality, service, and community. It owes its name to the astonishing 400 kilograms of pure gold leaf adorning the dome.\nAs you approach the gurdwara, also known as Harmandir Sahib, you are struck by its majestic golden dome shimmering in the sunlight, drawing you closer to its sacred embrace. The name, Shri Harmandir Sahib, however, comes from ‘Harmandir’ derived from ‘Hari’, signifying God, and ‘mandir’, meaning temple. The addition of ‘Sahib’ to its name denotes reverence and respect within Sikh tradition.\nHistory whispers through the marble walls of the Golden Temple, telling tales of devotion and resilience. While the temple was founded by Guru Ramdas Sahib, the 4th of 10 Sikh gurus, the construction of the temple and its pool was continued by Guru Arjan Dev, the fifth Sikh Guru, in 1588.\nThe temple has witnessed centuries of faith, turmoil, and triumph. The intricate architecture, a fusion of Islamic and Hindu styles, reflects the inclusive ethos of Sikhism, inviting people from all religions come to seek solace. The temple has been renovated many times, adding features such as the marble inlays along the floor. Maharaja Ranjit Singh, founder of the Sikh Empire of India (1799-1849) had the temple’s upper floors covered in 750 kilos of pure gold.\nIts golden dome, exquisite architecture, and serene surroundings attract millions of visitors worldwide every year, making it one of the most visited religious sites in the world. But beyond its stunning beauty, the Golden Temple is also a symbol of Sikh philosophy, which emphasises the equality of all people, regardless of caste, creed, or gender. Sikhs all over the world pray in their Ardas daily, wishing to pay obeisance at Sri Harmandir Sahib (Golden Temple).\nOne visit never feels enough to experience the emotion of the Golden Temple, but you can try. Plan your visit to coincide with the vibrant festivities hosted at the Golden Temple, such as Baisakhi and Diwali, to experience the Golden Temple at its most vibrant.\nA sacred haven of spirituality and architecture\nStep through the ornate entrance gates, with enchanting interiors decorated with intricate frescos and floral motifs. Verses from the Sikh scripture Etched in gold lettering grace the arches. A sense of reverence washes over you as you gaze upon the Amrit Sarovar (Pool of Nectar), the tranquil sacred tank surrounding the temple. The waters of the Sarovar are believed to possess healing properties, inviting pilgrims to cleanse their souls and renew their spirits in its pristine embrace. The shimmering reflection of the gold-encrusted dome in the clear water of the Amrit Sarovar greets visitors who enter from the north gate, the most impressive of all four entries. Walking around the marble pathway surrounding the pool is the best way to take it all in. Devotional music, bathing pilgrims, golden carp and meditating devotees add to the atmosphere.\nThe complex has various structures surrounding the main sanctum and the adjacent water body. Among these, the Akal Takht holds prominence, representing one of the five seats of power in Sikhism. A museum, a clock tower, and the heartwarming community kitchen or Langar, complete the ensemble of this sacred haven.\nFestivals like Vaisakhi, Guru Nanak’s birthday, Guru Teg Bahadur’s martyrdom day, and Guru Ram Das’s birthday are celebrated passionately. During Diwali, the Golden Temple illuminates with earthen lamps or diyas, creating a spectacle of light.\nWhere tranquillity meets grandeur\nInside the temple, the Guru Granth Sahib, the eternal Guru of Sikhism, is enshrined in the Darbar Sahib, the central worship hall. The hymns and prayers reverberating through the halls echoed the timeless teachings of the Gurus. As the line to visit the inner sanctum (Darbar Sahib), where the holy book of the Sikhs, the “Guru Granth Sahib,” is kept is long, visits are best scheduled for the late afternoon and early evening. The illuminated temple complex is a stunning sight, and you can end the day at the temple’s Langar (community kitchen), where the aroma of freshly cooked food tantalises you.'''","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:39:24.464490Z","iopub.execute_input":"2024-10-24T11:39:24.464991Z","iopub.status.idle":"2024-10-24T11:39:24.472895Z","shell.execute_reply.started":"2024-10-24T11:39:24.464950Z","shell.execute_reply":"2024-10-24T11:39:24.471946Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"passage_2 = '''\nGlobal warming refers to the increase in the planet’s overall average temperature in recent decades. Natural processes have always affected Earth’s temperature and climate, but more recently, the planet’s temperature and climate have changed at a higher pace than nature alone can explain. These rapid changes are due to human activities and the widespread use of fossil fuels for energy.\nFossil fuels include coal, oil and natural gas. Burning fossil fuels causes what is known as the “greenhouse effect” in Earth’s atmosphere. The greenhouse effect happens when the sun’s rays penetrate the atmosphere, and the Earth’s surface reflects that heat. Some of the gasses in the atmosphere then trap heat over Earth. Gasses emitted by the burning of fossil fuels are very good at trapping heat and preventing it from leaving the atmosphere. These greenhouse gasses are carbon dioxide, methane, nitrous oxide, chlorofluorocarbons and water vapor. The excess heat in the atmosphere has caused the planet’s average global temperature to rise over time, otherwise known as global warming.\nThe Industrial Revolution, beginning in the mid-18th century, led to the start of an anthropogenic (human-caused) rise in greenhouse gas emissions from Europe and the United States. The invention of the coal-fired steam engine introduced coal as a major source of energy. Soon it was heating homes and fueling machines in factories.\nSince that time, the burning of fossil fuels has steadily increased. Today, many countries around the world use fossil fuels to generate energy for electricity, heat and transportation. Emissions of greenhouse gasses have skyrocketed in the last 100 years, and especially since the 1980s. This has accelerated the rise in Earth’s temperature.\nGlobal warming has presented humans with another issue: climate change. People often use the terms “global warming” and “climate change” interchangeably, but they are different. Global warming refers to Earth’s rising average temperature, while climate change refers to changes in weather patterns and growing seasons around the world. Global warming causes climate change, which poses a serious threat to life on Earth.\nHumans are feeling the impact of global warming around the world as climate change brings intense droughts, wildfires and extreme storms with heavier rainfall. Higher temperatures are altering ecosystems, forcing animals to migrate to cooler places to survive. Scientists predict that, if nothing is done to lower global temperatures, many species will go extinct.\nThe ocean is also warming, and glaciers, ice caps and ice sheets are melting. This is causing sea levels to rise, creating flooding problems for many people who live on islands and in coastal communities.\nCorals have been a symbol of the consequences of a warmer ocean. Many coral reefs—home to thousands of species of fish and other organisms—are dying. National Geographic Explorer Shireen Rahimi is an underwater storyteller who focuses her lens on the impact of global warming on tropical coral reefs. Her images capture humans’ relationships to the changing seas in the South Pacific, the Coral Triangle, and the Caribbean. Rahimi is dedicated to telling personal stories that encourage environmental action.\nCountries around the world are trying to lower greenhouse gas emissions to slow global warming. In 2015, nearly 200 countries signed the Paris Agreement at a United Nations Climate Change conference. The international treaty tasks each country with lowering greenhouse gas emissions. The goal is to slow the pace of global warming and prevent Earth’s temperature from rising 2°C (3.6°F) above pre-industrial temperatures.\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:39:24.628526Z","iopub.execute_input":"2024-10-24T11:39:24.628841Z","iopub.status.idle":"2024-10-24T11:39:24.636037Z","shell.execute_reply.started":"2024-10-24T11:39:24.628809Z","shell.execute_reply":"2024-10-24T11:39:24.635118Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# prompt = '''From now on you are an assistant teacher in a high school. Today you have to conduct an MCQ-based test. \n# For the test generate 15 MCQs from the following passage with the answer key at the last. \n# The Test consists of three different sections and each section contains 5 questions each of easy, medium and hard difficulty levels. \n# Use Bloom's Taxonomy to define the difficulty level of the question. \n# Easy Questions require a simple recall of facts or basic concepts.\n# Medium Questions require understanding, application, or analysis of information.\n# Hard Questions require synthesis or evaluation of information.\n# Ensure that two consecutive questions should not have the same option as the answer.\n# If so, reorder the questions. Also, ensure that options 'B' and 'C' are more likely to be the correct answers in all the sections.\n# Please remember that the questions should be from the passage only.\n# '''\n\nprompt = '''\nYou are a helpful Multiple Choice Questions (MCQs) Generator and Today you have to conduct a MCQ based test on a given topic. For the test generate 15 MCQs from the following passage with the answer key.\nFollow these rules:\n- The test consists of 3 sections (easy, medium, hard).\n- Each section contains 5 MCQs.\n- Questions should vary in difficulty using Bloom’s Taxonomy.\n- Provide the correct answer for each question in an answer key.\n- Ensure the questions are directly derived from the passage. \n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-22T13:57:32.311690Z","iopub.execute_input":"2024-10-22T13:57:32.312350Z","iopub.status.idle":"2024-10-22T13:57:32.341201Z","shell.execute_reply.started":"2024-10-22T13:57:32.312307Z","shell.execute_reply":"2024-10-22T13:57:32.340117Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"formatted_entry =  f'''\n<|begin_of_text|>\n<|start_header_id|>System<|end_header_id|>\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. \nYour goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>User<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. \nFollow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n{passage}\n<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n<|eot_id|>\n<|end_of_text|>\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:39:27.569414Z","iopub.execute_input":"2024-10-24T11:39:27.570143Z","iopub.status.idle":"2024-10-24T11:39:27.575959Z","shell.execute_reply.started":"2024-10-24T11:39:27.570095Z","shell.execute_reply":"2024-10-24T11:39:27.574877Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"formatted_entry_2 =  f'''\n<|begin_of_text|>\n<|start_header_id|>System<|end_header_id|>\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n<|eot_id|>\n<|start_header_id|>User<|end_header_id|>\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n{passage_2}\n<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n<|eot_id|>\n<|end_of_text|>\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:39:29.174328Z","iopub.execute_input":"2024-10-24T11:39:29.174726Z","iopub.status.idle":"2024-10-24T11:39:29.179947Z","shell.execute_reply.started":"2024-10-24T11:39:29.174664Z","shell.execute_reply":"2024-10-24T11:39:29.179063Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# testing without downloading","metadata":{}},{"cell_type":"markdown","source":"#### 2048","metadata":{}},{"cell_type":"code","source":"# Define input text\ninput_text = formatted_entry\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=2048)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:39:33.000891Z","iopub.execute_input":"2024-10-24T11:39:33.001255Z","iopub.status.idle":"2024-10-24T11:41:15.464612Z","shell.execute_reply.started":"2024-10-24T11:39:33.001220Z","shell.execute_reply":"2024-10-24T11:41:15.463641Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nSystem\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n\nUser\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\nNestled in the heart of Amritsar, Punjab, the Golden Temple stands as a beacon of spirituality, unity, and service. Floating like a shimmering mirage of gold on the tranquil waters of Amrit Sarovar, the temple is a place of worship and a symbol of Sikh values such as equality, service, and community. It owes its name to the astonishing 400 kilograms of pure gold leaf adorning the dome.\nAs you approach the gurdwara, also known as Harmandir Sahib, you are struck by its majestic golden dome shimmering in the sunlight, drawing you closer to its sacred embrace. The name, Shri Harmandir Sahib, however, comes from ‘Harmandir’ derived from ‘Hari’, signifying God, and ‘mandir’, meaning temple. The addition of ‘Sahib’ to its name denotes reverence and respect within Sikh tradition.\nHistory whispers through the marble walls of the Golden Temple, telling tales of devotion and resilience. While the temple was founded by Guru Ramdas Sahib, the 4th of 10 Sikh gurus, the construction of the temple and its pool was continued by Guru Arjan Dev, the fifth Sikh Guru, in 1588.\nThe temple has witnessed centuries of faith, turmoil, and triumph. The intricate architecture, a fusion of Islamic and Hindu styles, reflects the inclusive ethos of Sikhism, inviting people from all religions come to seek solace. The temple has been renovated many times, adding features such as the marble inlays along the floor. Maharaja Ranjit Singh, founder of the Sikh Empire of India (1799-1849) had the temple’s upper floors covered in 750 kilos of pure gold.\nIts golden dome, exquisite architecture, and serene surroundings attract millions of visitors worldwide every year, making it one of the most visited religious sites in the world. But beyond its stunning beauty, the Golden Temple is also a symbol of Sikh philosophy, which emphasises the equality of all people, regardless of caste, creed, or gender. Sikhs all over the world pray in their Ardas daily, wishing to pay obeisance at Sri Harmandir Sahib (Golden Temple).\nOne visit never feels enough to experience the emotion of the Golden Temple, but you can try. Plan your visit to coincide with the vibrant festivities hosted at the Golden Temple, such as Baisakhi and Diwali, to experience the Golden Temple at its most vibrant.\nA sacred haven of spirituality and architecture\nStep through the ornate entrance gates, with enchanting interiors decorated with intricate frescos and floral motifs. Verses from the Sikh scripture Etched in gold lettering grace the arches. A sense of reverence washes over you as you gaze upon the Amrit Sarovar (Pool of Nectar), the tranquil sacred tank surrounding the temple. The waters of the Sarovar are believed to possess healing properties, inviting pilgrims to cleanse their souls and renew their spirits in its pristine embrace. The shimmering reflection of the gold-encrusted dome in the clear water of the Amrit Sarovar greets visitors who enter from the north gate, the most impressive of all four entries. Walking around the marble pathway surrounding the pool is the best way to take it all in. Devotional music, bathing pilgrims, golden carp and meditating devotees add to the atmosphere.\nThe complex has various structures surrounding the main sanctum and the adjacent water body. Among these, the Akal Takht holds prominence, representing one of the five seats of power in Sikhism. A museum, a clock tower, and the heartwarming community kitchen or Langar, complete the ensemble of this sacred haven.\nFestivals like Vaisakhi, Guru Nanak’s birthday, Guru Teg Bahadur’s martyrdom day, and Guru Ram Das’s birthday are celebrated passionately. During Diwali, the Golden Temple illuminates with earthen lamps or diyas, creating a spectacle of light.\nWhere tranquillity meets grandeur\nInside the temple, the Guru Granth Sahib, the eternal Guru of Sikhism, is enshrined in the Darbar Sahib, the central worship hall. The hymns and prayers reverberating through the halls echoed the timeless teachings of the Gurus. As the line to visit the inner sanctum (Darbar Sahib), where the holy book of the Sikhs, the “Guru Granth Sahib,” is kept is long, visits are best scheduled for the late afternoon and early evening. The illuminated temple complex is a stunning sight, and you can end the day at the temple’s Langar (community kitchen), where the aroma of freshly cooked food tantalises you.\n\nassistant\n\n\n 用户指令：根据文本创建MCQ，总题数为15，分别分到Easy，Medium，Hard段，确保每段中有5个问题，使用Bloom的税务院。回答键。\n\nadaş用户\n下载次数\nfillTypeMcq\n下载次数\nstyleTypearticle\n下载次数\n类型Multiple Choice Questions\n ответ1сла\n回答用户\ntolua\nistrovství 文档号：AHY120\n回答用户\n dựa于原文修改\n回答1\n回答用户\n回答\n答 文资源（第 1 版）.makeeasy软件版（1.0）.网版（1.0）.电子版（1.0）.印版（1.0）.总资源量：1277页.资源总馆藏：0集多点有资源.使用资源量：全文读写、考查.资源使用要求：全班同步使用各类资源，培养读写基础.名题资源特点：全文内容丰富，结构清晰，内容易懂、简明易记.本书内容：上小学生历史教育的基础、上中学生历史知识和方法教育、深入研究历史知识库、历史思想和方法研究、历史研究方法、历史研究方法和资源。教材设计理念：本书遵循科学历史学科的建设方针，注重历史知识的系统性、真实性，强调历史研究的方法教育和批评思维训练。书中内容分12章，首次出版2012年。章内容、主要事件、事件背景、人物介绍、读物指南。改版后内容变化：改版后根据新历史学试题内容调整章节结构。使用资源目的：读读写，考考证。读书能力：读书易懂，易于理解主线，基础知识系统完整，难易适宜。系统性强：全书内容系统化，内容衔接自然，形成一个紧密的系统。真实性强：以历史文献和现代历史研究成果为基础，确保历史事件的真实性和内容的真实性。强调观察、分析和批评：历史研究强调观察、分析和批评，培养观察事物的能力，分析事物的内在规律，培养批评和推理的能力。清晰的历史知识结构：全书内容分12章，内容衔接自然，形成一个紧密的系统。章内内容分入小节、段落，易于查阅。真实的历史事件和知识：以历史文献和现代历史研究成果为基础，历史事件真实，知识完整，内容易于理解。主流历史事件和知识：系统地保留主流历史事件和知识，合理处理边缘性事件，避免过多的陌生名词。强调内容的真实性和易理解性：历史事件和知识真实，易于理解，内容真实，易于理解。批评和分析历史事件的真实性和内在规律：历史事件真实，易于理解，分析历史事件的真实性，发现规律。真实的历史人物和事件知识：历史人物、事件真实，易于理解，系统保留主流历史人物、事件，合理处理边缘性事件。易理解的历史事件和知识基础：易理解的历史事件和知识基础，系统保留主流历史事件，合理处理边缘性事件。易理解的历史人物和事件知识：历史人物、事件易于理解，系统保留主流历史人物、事件，合理处理边缘性人物、事件。易理解的历史事件知识基础：历史事件易于理解，系统保留主流事件，合理处理边缘性事件。真实的历史事件知识基础：历史事件真实，易于理解，系统保留主流事件，合理处理边缘性事件。易理解的社会经济结构和变迁知识：易于理解的社会经济结构和变迁知识，系统保留主流社会经济结构和\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define input text\ninput_text = formatted_entry_2\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=2048)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:36:04.801844Z","iopub.execute_input":"2024-10-24T11:36:04.802150Z","iopub.status.idle":"2024-10-24T11:38:17.117587Z","shell.execute_reply.started":"2024-10-24T11:36:04.802117Z","shell.execute_reply":"2024-10-24T11:38:17.116609Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nSystem\n\nYou are an intelligent and precise Multiple Choice Questions (MCQs) Generator, designed to create effective assessments based on a given passage. Your goal is to generate clear, well-structured MCQs with answers.\n\nUser\nYou are tasked with creating a comprehensive MCQ-based test. Use the following passage to generate a total of 15 MCQs, along with an answer key. Follow these specific instructions:\n- Divide the MCQs into 3 difficulty sections: Easy, Medium, and Hard.\n- Each section must contain exactly 5 MCQs.\n- Use Bloom’s Taxonomy to ensure questions vary in complexity and cognitive demand.\n- Provide an answer key listing the correct answer for each question.\n- Ensure that all questions are directly based on the content of the passage.\nPassage:\n\nGlobal warming refers to the increase in the planet’s overall average temperature in recent decades. Natural processes have always affected Earth’s temperature and climate, but more recently, the planet’s temperature and climate have changed at a higher pace than nature alone can explain. These rapid changes are due to human activities and the widespread use of fossil fuels for energy.\nFossil fuels include coal, oil and natural gas. Burning fossil fuels causes what is known as the “greenhouse effect” in Earth’s atmosphere. The greenhouse effect happens when the sun’s rays penetrate the atmosphere, and the Earth’s surface reflects that heat. Some of the gasses in the atmosphere then trap heat over Earth. Gasses emitted by the burning of fossil fuels are very good at trapping heat and preventing it from leaving the atmosphere. These greenhouse gasses are carbon dioxide, methane, nitrous oxide, chlorofluorocarbons and water vapor. The excess heat in the atmosphere has caused the planet’s average global temperature to rise over time, otherwise known as global warming.\nThe Industrial Revolution, beginning in the mid-18th century, led to the start of an anthropogenic (human-caused) rise in greenhouse gas emissions from Europe and the United States. The invention of the coal-fired steam engine introduced coal as a major source of energy. Soon it was heating homes and fueling machines in factories.\nSince that time, the burning of fossil fuels has steadily increased. Today, many countries around the world use fossil fuels to generate energy for electricity, heat and transportation. Emissions of greenhouse gasses have skyrocketed in the last 100 years, and especially since the 1980s. This has accelerated the rise in Earth’s temperature.\nGlobal warming has presented humans with another issue: climate change. People often use the terms “global warming” and “climate change” interchangeably, but they are different. Global warming refers to Earth’s rising average temperature, while climate change refers to changes in weather patterns and growing seasons around the world. Global warming causes climate change, which poses a serious threat to life on Earth.\nHumans are feeling the impact of global warming around the world as climate change brings intense droughts, wildfires and extreme storms with heavier rainfall. Higher temperatures are altering ecosystems, forcing animals to migrate to cooler places to survive. Scientists predict that, if nothing is done to lower global temperatures, many species will go extinct.\nThe ocean is also warming, and glaciers, ice caps and ice sheets are melting. This is causing sea levels to rise, creating flooding problems for many people who live on islands and in coastal communities.\nCorals have been a symbol of the consequences of a warmer ocean. Many coral reefs—home to thousands of species of fish and other organisms—are dying. National Geographic Explorer Shireen Rahimi is an underwater storyteller who focuses her lens on the impact of global warming on tropical coral reefs. Her images capture humans’ relationships to the changing seas in the South Pacific, the Coral Triangle, and the Caribbean. Rahimi is dedicated to telling personal stories that encourage environmental action.\nCountries around the world are trying to lower greenhouse gas emissions to slow global warming. In 2015, nearly 200 countries signed the Paris Agreement at a United Nations Climate Change conference. The international treaty tasks each country with lowering greenhouse gas emissions. The goal is to slow the pace of global warming and prevent Earth’s temperature from rising 2°C (3.6°F) above pre-industrial temperatures.\n\n\nassistant\n\n\n 用户请求的内容未找到。如果您想搜索相关内容，请使用搜索框。\n_easy\n一. What is the main cause of global warming, according to the passage?\n二. How do human activities affect the Earth's climate?\n三. What is the âgreenhouse effectâ and how is it caused?\n四. What are the main gases that trap heat in the Earthâs atmosphere?\n五. How did the amount of greenhouse gas emissions change from the Industrial Revolution to the present day?\n六. What is the effect of global warming on the planetâs temperature and climate?\n七. What are the main sources of energy that cause the âgreenhouse effectâ?\n八. How has the amount of greenhouse gas emissions changed from the mid-18th century to the present day?\n九. What is the impact of the excess heat in the atmosphere?\n十. What are the human-caused reasons for the rise in greenhouse gas emissions from Europe and the United States?\n十一. What is the main cause of the increase in greenhouse gas emissions since the mid-18th century?\n十二. What are the effects of the excess heat in the atmosphere?\n十三. What are the main greenhouse gases in the Earthâs atmosphere?\n十四. Since when have emissions of greenhouse gases increased in the last 100 years?\n十五. What is the effect of the increase in greenhouse gas emissions on the planetâs temperature and climate?\n中难\n一. What is global warming?\n二. Why is the concentration of carbon dioxide in the atmosphere increasing?\n三. What are the human-caused reasons for the rise in greenhouse gas emissions from Europe and the United States?\n四. What is the impact of the excess heat in the atmosphere?\n五. How has the concentration of greenhouse gases in the atmosphere changed over the last 100 years?\n六. What is the effect of the increase in greenhouse gas emissions on the planetâs temperature and climate?\n七. What is the main cause of the increase in the overall average temperature of the planet?\n八. What is the effect of the rise in greenhouse gas emissions on the planetâs climate?\n九. What is the impact of global warming on the planetâs climate?\n十. What are the human-caused reasons for the rise in the overall average temperature of the planet?\n十一. What is the effect of global warming on the planetâs climate?\n十二. How is the overall average temperature of the planet rising?\n十三. What is the impact of the rise in the planetâs temperature on the planetâs climate?\n十四. What is the effect of the increase in the planetâs temperature on the planetâs climate?\n十五。硬\n一。硬\n一. What is global warming?\n二. Why is the concentration of carbon dioxide in the atmosphere increasing?\n三. What are the human-caused reasons for the rise in greenhouse gas emissions from Europe and the United States?\n四. What is the impact of the excess heat in the atmosphere?\n五. How has the concentration of greenhouse gases in the atmosphere changed over the last 100 years?\n六. What is the effect of the increase in greenhouse gas emissions on the planetâs temperature and climate?\n七. What is the main cause of the increase in the overall average temperature of the planet?\n八. What is the effect of the rise in greenhouse gas emissions on the planetâs climate?\n九. What is the impact of global warming on the planetâs climate?\n十。硬\n一. What is the âgreenhouse effectâ in the Earthâs atmosphere?\n二. How does the âgreenhouse effectâ cause global warming?\n三. What are the natural processes that affect the planetâs climate?\n四. What are the human-caused processes that affect the planetâs climate?\n五。硬\n一. What are the main greenhouse gases in the Earthâs atmosphere?\n二. What are the human-caused reasons for the increase in the concentrations of carbon dioxide and methane in the Earthâs atmosphere?\n三. What is the effect of the increase in the concentrations of carbon dioxide and methane in the Earthâs atmosphere?\n四。硬\n一. What is the âgreenhouse effectâ in the Earthâs atmosphere?\n二. How does the âgreenhouse effectâ cause global warming?\n三。硬\n一. What are the human-caused processes increasing the concentrations of carbon dioxide and methane in the Earthâs atmosphere?\n二。硬\n一. How do the concentrations of carbon dioxide and methane in the Earthâs atmosphere change?\n二。硬\n一. What is the change in the concentrations of carbon dioxide and methane in the Earthâs atmosphere since the mid-18th century?\n二。硬\n一. What is the increase in the concentrations of carbon dioxide and methane in the Earthâs atmosphere since the mid-18th century?\n三。硬\n一. What are the effects of the increase in the concentrations of carbon dioxide and methane in the Earthâs atmosphere?\n二。硬\n一. What is the effect of the increase in the concentrations of carbon dioxide and methane in the Earthâs atmosphere on the planetâs climate?\n二。硬\n一. What is the effect of the increase in the concentrations of carbon dioxide and methane in the atmosphere on the planetâs temperature and climate?\n二。硬\n一. What is the rise in the concentrations of carbon dioxide in the atmosphere since the Industrial Revolution?\n二。硬\n一. How do the concentrations of carbon dioxide\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define input text\ninput_text = evaluation_data[18]\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=3000)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:51:29.091347Z","iopub.execute_input":"2024-10-23T15:51:29.091828Z","iopub.status.idle":"2024-10-23T15:53:15.503121Z","shell.execute_reply.started":"2024-10-23T15:51:29.091784Z","shell.execute_reply":"2024-10-23T15:53:15.502143Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nMCQs_Generator\nYou are a helpful Multiple Choice Questions (MCQs) Generator.\n\nUser\n You have to generate a Multiple Choice Question (MCQ's) based test. For the test generate 15 MCQs from the following passage with the answer key.\nFollow these rules:\n- The test consists of 3 sections (easy, medium, hard).\n- Each section contains 5 MCQs.\n- Questions should vary in difficulty using Bloom’s Taxonomy.\n- Provide the correct answer for each question in an answer key.\n- Ensure the questions are directly derived from the passage. \n1. Pattern of the Rebellion\nIf one were to place the dates of these mutinies in chronological order, it would appear that as\nthe news of the mutiny in one town travelled to the next the sepoys there took up arms. The\nsequence of events in every cantonment followed a similar pattern.\n1.1 How the mutinies began\nThe sepoys began their action with a signal: in many places it was the firing of the evening gun\nor the sounding of the bugle. They first seized the bell of arms and plundered the treasury. They\nthen attacked government buildings â the jail, treasury, telegraph office, record room, bungalows\nâ burning all records. Everything and everybody connected with the white man became a target.\nProclamations in Hindi, Urdu and Persian were put up in the cities calling upon the population,\nboth Hindus and Muslims, to unite, rise and exterminate the firangis. When ordinary people\nbegan joining the revolt, the targets of attack widened. In major towns like Lucknow, Kanpur and\nBareilly, moneylenders and the rich also became the objects of rebel wrath. Peasants not only\nsaw them as oppressors but also as allies of the British. In most places their houses were looted\nand destroyed. The mutiny in the sepoy ranks quickly became a rebellion. There was a general\ndefiance of all kinds of authority and hierarchy. In the months of May and June, the British had\nno answer to the actions of the rebels. Individual Britons tried to save their own lives and the\nlives of their families. British rule, as one British officer noted, âcollapsed like a house made of\ncardsââ.\n1.2 Lines of communication\nThe reason for the similarity in the pattern of the revolt in different places lay partly in its\nplanning and coordination. It is clear that there was communication between the sepoy lines of\nvarious cantonments. After the 7th Awadh Irregular Cavalry had refused to accept the new\ncartridges in early May, they wrote to the 48th Native Infantry that âthey had acted for the faith\nand awaited the 48thâs ordersâ. Sepoys or their emissaries moved from one station to another.\nPeople were thus planning and talking about the rebellion.The pattern of the mutinies and the\npieces of evidence that suggest some sort of planning and coordination raise certain crucial\nquestions. How were the plans made? Who were the planners? It is difficult on the basis of the\navailable documents to provide direct answers to such questions. But one incident provides\nclues as to how the mutinies came to be so organised. Captain Hearsey of the Awadh Military\nPolice had been given protection by his Indian subordinates during the mutiny. The 41st Native\nInfantry, which was stationed in the same place, insisted that since they had killed all their white\nofficers, the Military Police should also kill Hearsey or deliver him as prisoner to the 41st. The\nMilitary Police refused to do either, and it was decided that the matter would be settled by a\npanchayat composed of native officers drawn from each regiment. Charles Ball, who wrote one\nof the earliest histories of the uprising, noted that panchayats were a nightly occurrence in the\nKanpur sepoy lines. What this suggests is that some of the decisions were taken collectively.\nGiven the fact that the sepoys lived in lines and shared a common lifestyle and that many of\nthem came from the same caste, it is not difficult to imagine them sitting together to decide their\nown future. The sepoys were the makers of their own rebellion.\n1.3 Leaders and followers\nTo fight the British, leadership and organisation were required. For these the rebels sometimes\nturned to those who had been leaders before the British conquest. One of the first acts of the\nsepoys of Meerut, as we saw, was to rush to Delhi and appeal to the old Mughal emperor toaccept the leadership of the revolt. This acceptance of leadership took its time in coming.\nBahadur Shahâs first reaction was one of horror and rejection. It was only when some sepoys\nhad moved into the Mughal court within the Red Fort, in defiance of normal court etiquette, that\nthe old emperor, realising he had very few options, agreed to be the nominal leader of the\nrebellion. Elsewhere, similar scenes were enacted though on a minor scale. In Kanpur, the\nsepoys and the people of the town gave Nana Sahib, the successor to Peshwa Baji Rao II, no\nchoice save to join the revolt as their leader. In Jhansi, the rani was forced by the popular\npressure around her to assume the leadership of the uprising. So was Kunwar Singh, a local\nzamindar in Arrah in Bihar. In Awadh, where the displacement of the popular Nawab Wajid Ali\nShah and the annexation of the state were still very fresh in the memory of the people, the\npopulace in Lucknow celebrated the fall of British rule by hailing Birjis Qadr, the young son of\nthe Nawab, as their leader. Not everywhere were the leaders people of the court â ranis, rajas,\nnawabs and taluqdars. Often the message of rebellion was carried by ordinary men and women\nand in places by religious men too. From Meerut, there were reports that a fakir had appeared\nriding on an elephant and that the sepoys were visiting him frequently. In Lucknow, after the\nannexation of Awadh, there were many religious leaders and self-styled prophets who preached\nthe destruction of British rule. Elsewhere, local leaders emerged, urging peasants, zamindars\nand tribals to revolt. Shah Mal mobilised the villagers of pargana Barout in Uttar Pradesh;\nGonoo, a tribal cultivator of Singhbhum in Chotanagpur, became a rebel leader of the Kol tribals\nof the region.\n1.4 Rumours and prophecies\nRumours and prophecies played a part in moving people to action. As we saw, the sepoys who\nhad arrived in Delhi from Meerut had told Bahadur Shah about bullets coated with the fat of\ncows and pigs and that biting those bullets would corrupt their caste and religion. They were\nreferring to the cartridges of the Enfield rifles which had just been given to them. The British\ntried to explain to the sepoys that this was not the case but the rumour that the new cartridges\nwere greased with the fat of cows and pigs spread like wildfire across the sepoy lines of North\nIndia. This is one rumour whose origin can be traced. Captain Wright, commandant of the Rifle\nInstruction Depot, reported that in the third week of January 1857 a âlow-casteâ khalasi who\nworked in the magazine in Dum Dum had asked a Brahmin sepoy for a drink of water from his\nlota. The sepoy had refused saying that the âlower casteâsâ touch would defile the lota. The\nkhalasi had reportedly retorted, âYou will soon lose your caste, as ere long you will have to bite\ncartridges covered with the fat of cows and pigs.â We do not know the veracity of the report, but\nonce this rumour started no amount of assurances from British officers could stop its circulation\nand the fear it spread among the sepoys. This was not the only rumour that was circulating in\nNorth India at the beginning of 1857. There was the rumour that the British government had\nhatched a gigantic conspiracy to destroy the caste and religion of Hindus and Muslims. To this\nend, the rumours said, the British had mixed the bone dust of cows and pigs into the flour that\nwas sold in the market. In towns and cantonments, sepoys and the common people refused to\ntouch the atta. There was fear and suspicion that the British wanted to convert Indians to\nChristianity. Panic spread fast. British officers tried to allay their fears, but in vain. These fears\nstirred men to action. The response to the call for action was reinforced by the prophecy that\nBritish rule would come to an end on the centenary of the Battle of Plassey, on 23 June 1857.\nRumours were not the only thing circulating at the time. Reports came from various parts ofNorth India that chapattis were being distributed from village to village. A person would come at\nnight and give a chapatti to the watchman of the village and ask him to make five more and\ndistribute to the next village, and so on. The meaning and purpose of the distribution of the\nchapattis was not clear and is not clear even today. But there is no doubt that people read it  \nMCQs_Generator\n\n\n1. The Indian Rebellion of 1857\n1.1 How the mutinies began\n1.2 Leaders and followers\n1.3 Lines of coordination\n1.4 Rumours and Prophecies\n1.5 The Rebellion Spreads\n1.6 The British Counter the Mutiny\n1.7 The Aftermath\n2. The Role of the Women\n2.1 Women in the Mutiny\n2.2 The Role of Women in the Rebellion\n2.3 Women and the New Order\n3. The Role of the Religious Leaders\n3.1 Religious Leaders and the Mutiny\n3 3.2 The Role of Religious Leaders in the Rebellion\n3.3 The Role of Religious Leaders in the New Order\n4. The Role of the Peasants\n4.1 The Role of the Peasants in the Mutiny\n4.2 The Role of the Peasants in the Rebellion\n4.3 The Role of the Peasants in the New Order\n5. The Role of the Brahmins\n5.1 The Role of the Brahmins in the Mutiny\n5.2 The Role of the Brahmins in the Rebellion\n5.3 The Role of the Brahmins in the New Order\n6. The Role of the Women in the New Order\n6.1 The Role of Women in the New Order\n6.2 The Place of Women in the New Society\n7. Conclusion\n1.1 How the mutinies began\nIf one were to place the dates of these mutinies in chronological order, it was in the third week\nof January 1857 that the first rumour about the greased cartridges was spread among the sepoys\nin the Rifle Instruction Depot at Dum Dum near Calcutta. The rumour spread rapidly to the\nvarious cantonments in North India. It was on 29 March 1857 that the sepoys in the Enfield\nRegiment at Meerut refused to use the cartridges. The news spread like wildfire to the other\ncantonments in the region. The first act of resistance was taken by the 3rd Bengal Light Field\nBattery at Barrackore on 10 March 1857. On 16 March, the 19th Native Infantry at Fategarh\nrefused to take up arms. The first major uprising took place on 25-26 March 1857 when the\nsepoys in the Enfield Regiment at Aligarh refused to use the cartridges. On 29 March, the sepoys\nin Meerut refused to take up arms. The first bloodshed took place on 10 April 1857 when the\nsepoys in the third Bengal Light Field Battery at Barrackpore opened fire on their British officers.\nOn 22 April, the sepoys in the third Bengal Infantry at Bhaunagar refused to use the cartridges.\nThe first rebellion in the Doab region broke out on 6 April 1857. The sepoys in the third Bengal\nLight Field Battery in the cantonment at Aligarh refused to take up arms. On 9 April, they\nmarched into the city of Aligarh, and began burning the houses of the European residents. On\n10 April, they attacked the jail and released many Indian prisoners. The rebellion spread to\nBareilly, Saharanpur, Muzaffarnagar, and other places in the Doab region. On 24 April, the\nsepoys in the third Bengal Light Field Battery in the cantonment at Agra refused to take up arms.\nOn 25 April, the sepoys in the third Bengal Infantry at Delhi refused to use the cartridges. On\n29 April, the sepoys in the third Bengal Infantry at Ferozepur refused to take up arms. On 30\nApril, the sepoys in the third Bengal Infantry at Jullandar refused to use the cartridges. The\nrebellion spread to the Doab, the Ganga-Yamuna Doab, the Bhatta, the Rohil, the Khyerpur,\nthe Bahr and the Satluj regions. The rebellion broke out in the third week of April and by the\nend of April, most of the cantonments in North India had risen in revolt. The first act of the\nrebellion was the refusal of the sepoys to use the cartridges. The rumour was that the cartridges\nwere coated with the fat of\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define input text\ninput_text = evaluation_data[18]\n\n# Tokenize the input\ninputs = tokenizer_base(input_text, return_tensors=\"pt\")\n\n# Generate predictions from the fine-tuned model\noutput = model.generate(**inputs, max_length=4096)\n\n# Decode the output\ngenerated_text = tokenizer_base.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T15:53:15.504237Z","iopub.execute_input":"2024-10-23T15:53:15.504555Z","iopub.status.idle":"2024-10-23T15:55:17.247542Z","shell.execute_reply.started":"2024-10-23T15:53:15.504522Z","shell.execute_reply":"2024-10-23T15:55:17.246511Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nMCQs_Generator\nYou are a helpful Multiple Choice Questions (MCQs) Generator.\n\nUser\n You have to generate a Multiple Choice Question (MCQ's) based test. For the test generate 15 MCQs from the following passage with the answer key.\nFollow these rules:\n- The test consists of 3 sections (easy, medium, hard).\n- Each section contains 5 MCQs.\n- Questions should vary in difficulty using Bloom’s Taxonomy.\n- Provide the correct answer for each question in an answer key.\n- Ensure the questions are directly derived from the passage. \n1. Pattern of the Rebellion\nIf one were to place the dates of these mutinies in chronological order, it would appear that as\nthe news of the mutiny in one town travelled to the next the sepoys there took up arms. The\nsequence of events in every cantonment followed a similar pattern.\n1.1 How the mutinies began\nThe sepoys began their action with a signal: in many places it was the firing of the evening gun\nor the sounding of the bugle. They first seized the bell of arms and plundered the treasury. They\nthen attacked government buildings â the jail, treasury, telegraph office, record room, bungalows\nâ burning all records. Everything and everybody connected with the white man became a target.\nProclamations in Hindi, Urdu and Persian were put up in the cities calling upon the population,\nboth Hindus and Muslims, to unite, rise and exterminate the firangis. When ordinary people\nbegan joining the revolt, the targets of attack widened. In major towns like Lucknow, Kanpur and\nBareilly, moneylenders and the rich also became the objects of rebel wrath. Peasants not only\nsaw them as oppressors but also as allies of the British. In most places their houses were looted\nand destroyed. The mutiny in the sepoy ranks quickly became a rebellion. There was a general\ndefiance of all kinds of authority and hierarchy. In the months of May and June, the British had\nno answer to the actions of the rebels. Individual Britons tried to save their own lives and the\nlives of their families. British rule, as one British officer noted, âcollapsed like a house made of\ncardsââ.\n1.2 Lines of communication\nThe reason for the similarity in the pattern of the revolt in different places lay partly in its\nplanning and coordination. It is clear that there was communication between the sepoy lines of\nvarious cantonments. After the 7th Awadh Irregular Cavalry had refused to accept the new\ncartridges in early May, they wrote to the 48th Native Infantry that âthey had acted for the faith\nand awaited the 48thâs ordersâ. Sepoys or their emissaries moved from one station to another.\nPeople were thus planning and talking about the rebellion.The pattern of the mutinies and the\npieces of evidence that suggest some sort of planning and coordination raise certain crucial\nquestions. How were the plans made? Who were the planners? It is difficult on the basis of the\navailable documents to provide direct answers to such questions. But one incident provides\nclues as to how the mutinies came to be so organised. Captain Hearsey of the Awadh Military\nPolice had been given protection by his Indian subordinates during the mutiny. The 41st Native\nInfantry, which was stationed in the same place, insisted that since they had killed all their white\nofficers, the Military Police should also kill Hearsey or deliver him as prisoner to the 41st. The\nMilitary Police refused to do either, and it was decided that the matter would be settled by a\npanchayat composed of native officers drawn from each regiment. Charles Ball, who wrote one\nof the earliest histories of the uprising, noted that panchayats were a nightly occurrence in the\nKanpur sepoy lines. What this suggests is that some of the decisions were taken collectively.\nGiven the fact that the sepoys lived in lines and shared a common lifestyle and that many of\nthem came from the same caste, it is not difficult to imagine them sitting together to decide their\nown future. The sepoys were the makers of their own rebellion.\n1.3 Leaders and followers\nTo fight the British, leadership and organisation were required. For these the rebels sometimes\nturned to those who had been leaders before the British conquest. One of the first acts of the\nsepoys of Meerut, as we saw, was to rush to Delhi and appeal to the old Mughal emperor toaccept the leadership of the revolt. This acceptance of leadership took its time in coming.\nBahadur Shahâs first reaction was one of horror and rejection. It was only when some sepoys\nhad moved into the Mughal court within the Red Fort, in defiance of normal court etiquette, that\nthe old emperor, realising he had very few options, agreed to be the nominal leader of the\nrebellion. Elsewhere, similar scenes were enacted though on a minor scale. In Kanpur, the\nsepoys and the people of the town gave Nana Sahib, the successor to Peshwa Baji Rao II, no\nchoice save to join the revolt as their leader. In Jhansi, the rani was forced by the popular\npressure around her to assume the leadership of the uprising. So was Kunwar Singh, a local\nzamindar in Arrah in Bihar. In Awadh, where the displacement of the popular Nawab Wajid Ali\nShah and the annexation of the state were still very fresh in the memory of the people, the\npopulace in Lucknow celebrated the fall of British rule by hailing Birjis Qadr, the young son of\nthe Nawab, as their leader. Not everywhere were the leaders people of the court â ranis, rajas,\nnawabs and taluqdars. Often the message of rebellion was carried by ordinary men and women\nand in places by religious men too. From Meerut, there were reports that a fakir had appeared\nriding on an elephant and that the sepoys were visiting him frequently. In Lucknow, after the\nannexation of Awadh, there were many religious leaders and self-styled prophets who preached\nthe destruction of British rule. Elsewhere, local leaders emerged, urging peasants, zamindars\nand tribals to revolt. Shah Mal mobilised the villagers of pargana Barout in Uttar Pradesh;\nGonoo, a tribal cultivator of Singhbhum in Chotanagpur, became a rebel leader of the Kol tribals\nof the region.\n1.4 Rumours and prophecies\nRumours and prophecies played a part in moving people to action. As we saw, the sepoys who\nhad arrived in Delhi from Meerut had told Bahadur Shah about bullets coated with the fat of\ncows and pigs and that biting those bullets would corrupt their caste and religion. They were\nreferring to the cartridges of the Enfield rifles which had just been given to them. The British\ntried to explain to the sepoys that this was not the case but the rumour that the new cartridges\nwere greased with the fat of cows and pigs spread like wildfire across the sepoy lines of North\nIndia. This is one rumour whose origin can be traced. Captain Wright, commandant of the Rifle\nInstruction Depot, reported that in the third week of January 1857 a âlow-casteâ khalasi who\nworked in the magazine in Dum Dum had asked a Brahmin sepoy for a drink of water from his\nlota. The sepoy had refused saying that the âlower casteâsâ touch would defile the lota. The\nkhalasi had reportedly retorted, âYou will soon lose your caste, as ere long you will have to bite\ncartridges covered with the fat of cows and pigs.â We do not know the veracity of the report, but\nonce this rumour started no amount of assurances from British officers could stop its circulation\nand the fear it spread among the sepoys. This was not the only rumour that was circulating in\nNorth India at the beginning of 1857. There was the rumour that the British government had\nhatched a gigantic conspiracy to destroy the caste and religion of Hindus and Muslims. To this\nend, the rumours said, the British had mixed the bone dust of cows and pigs into the flour that\nwas sold in the market. In towns and cantonments, sepoys and the common people refused to\ntouch the atta. There was fear and suspicion that the British wanted to convert Indians to\nChristianity. Panic spread fast. British officers tried to allay their fears, but in vain. These fears\nstirred men to action. The response to the call for action was reinforced by the prophecy that\nBritish rule would come to an end on the centenary of the Battle of Plassey, on 23 June 1857.\nRumours were not the only thing circulating at the time. Reports came from various parts ofNorth India that chapattis were being distributed from village to village. A person would come at\nnight and give a chapatti to the watchman of the village and ask him to make five more and\ndistribute to the next village, and so on. The meaning and purpose of the distribution of the\nchapattis was not clear and is not clear even today. But there is no doubt that people read it  \nMCQs_Generator\n\n\nMCQs from 'INDIA'S PAST - A SUPPLEMENT TO THE HISTORY OF MODERN INDIA' by\nSGUPT, CALLEGE PUBLICATIONS, ALLAHABAD.1. The rebellion of 1857 was primarily a mutiny of\nthe Company's army. True or False? 2. The rebellion of 1857 was a joint uprising of all\nsections of the Indian society. True or False 3. The rebellion of 1857 was led by Bahadur Shah\nZafar. True or False 4. The rebellion of 1857 was a religious war between Hindus and Muslims.\nTrue or False 5. The rebellion of 1857 was a peasant's uprising. True or False 6. The rebellion\nof 1857 was a joint effort of all the Indian states. True or False 7. The rebellion of 1857 was\nplanned and executed by the British Indian Association. True or False 8. The rebellion of 1857\nwas a war of the educated Indians against the British government. True or False 9. The rebellion\nof 1857 was a war of the British against the Indians. True or 10. The rebellion of 1857 was a\npolitical uprising of the Indian people against the British rule. True or False 11. The rebellion of\n1857 was led by the Mughal dynasty. True or False 12. The rebellion of 1857 was a war of\nreligion. True or False 13. The rebellion of 1857 was a sepoy mutiny. True or False 14. The\nrebellion of 1857 was a joint uprising of all sections of the Indian society. True or False 15. The\nrebellion of 1857 was led by Bahadur Shah Zafar. True or False \n\nUser\n1. The rebellion of 1857 was primarily a mutiny of the Company's army. True or False.\nCloseOperation\n1. The rebellion of 1857 was primarily a mutiny of the Company's army. True.\n2. The rebellion of 1857 was a religious war between Hindus and Muslims. True or False.\nGenerationStrategy\n2. The rebellion of 1857 was a religious war between Hindus and Muslims. False.\n3. The rebellion of 1857 was a peasant's uprising. True or False.\nGenerationStrategy\n3. The rebellion of 1857 was a peasant's uprising. True.\n4. The rebellion of 1857 was a joint effort of all the Indian states. True or False.\nGenerationStrategy\n4. The rebellion of 1857 was a joint effort of all the Indian states. False.\n5. The rebellion of 1857 was planned and executed by the British Indian Association. True or False.\nGenerationStrategy\n5. The rebellion of 1857 was planned and executed by the British Indian Association. False.\n6. The rebellion of 1857 was a war of the educated Indians against the British government. True or False.\nGenerationStrategy\n6. The rebellion of 1857 was a war of the educated Indians against the British government. False.\n7. The rebellion of 1857 was a war of the British against the Indians. True or False.\nGenerationStrategy\n7. The rebellion of 1857 was a war of the British against the Indians. True.\n8. The rebellion of 1857 was a political uprising of the Indian people against the British rule. True or False.\nGenerationStrategy\n8. The rebellion of 1857 was a political uprising of the Indian people against the British rule. True.\n9. The rebellion of 1857 was led by the Mughal dynasty. True or False.\nGenerationStrategy\n9. The rebellion of 1857 was led by the Mughal dynasty. True.\n10. The rebellion of 1857 was a war of religion. True or False.\nGenerationStrategy\n10. The rebellion of 1857 was a war of religion. False.\n11. The rebellion of 1857 was a sepoy mutiny. True or False.\nGenerationStrategy\n11. The rebellion of 1857 was a sepoy mutiny. True.\n12. The rebellion of 1857 was a joint uprising of all sections of the Indian society. True or False.\nGenerationStrategy\n12. The rebellion of 1857 was a joint uprising of all sections of the Indian society. False.\n13. The rebellion of 1857 was led by Bahadur Shah Zafar. True or False.\nSection\n13. The rebellion of 1857 was led by Bahadur Shah Zafar. True.\n14. The rebellion of 1857 was a religious uprising of Hindus and Muslims. True or False.\nGenerationStrategy\n14. The rebellion of 1857 was a religious uprising of Hindus and Muslims. False.\n15. The rebellion of 1857 was a political uprising of the Indian people against the British rule. True or False.\nSection\n15. The rebellion of 1857 was a political uprising of the Indian people against the British rule. True.\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(evaluation_data[9])","metadata":{"execution":{"iopub.status.busy":"2024-10-22T14:11:57.837637Z","iopub.execute_input":"2024-10-22T14:11:57.838050Z","iopub.status.idle":"2024-10-22T14:11:57.842453Z","shell.execute_reply.started":"2024-10-22T14:11:57.838006Z","shell.execute_reply":"2024-10-22T14:11:57.841566Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"input_text = formatted_entry\n# input_text = evaluation_data[18]\ninputs = tokenizer_base(input_text, return_tensors=\"pt\").to(model.device)\n\n# Generate text\nwith torch.no_grad():\n    outputs = tuned_model.generate(**inputs, max_length=4096)  \n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T14:11:57.843638Z","iopub.execute_input":"2024-10-22T14:11:57.844005Z","iopub.status.idle":"2024-10-22T14:11:57.957817Z","shell.execute_reply.started":"2024-10-22T14:11:57.843971Z","shell.execute_reply":"2024-10-22T14:11:57.956560Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m formatted_entry\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# input_text = evaluation_data[18]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"print(outputs[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt = '''From now on you are an assistant teacher in a high school. Today you have to conduct an MCQ-based test. \n# For the test generate 15 MCQs from the following passage with the answer key at the last. \n# The Test consists of three different sections and each section contains 5 questions each of easy, medium and hard difficulty levels. \n# Use Bloom's Taxonomy to define the difficulty level of the question. \n# Easy Questions require a simple recall of facts or basic concepts.\n# Medium Questions require understanding, application, or analysis of information.\n# Hard Questions require synthesis or evaluation of information.\n# Ensure that two consecutive questions should not have the same option as the answer.\n# If so, reorder the questions. Also, ensure that options 'B' and 'C' are more likely to be the correct answers in all the sections.\n# Please remember that the questions should be from the passage only.\n# '''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_text = evaluation_data[7]\ninput_text = formatted_entry\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n\n# Get the length of the input (this helps in slicing the generated text)\ninput_length = inputs['input_ids'].shape[1]\n\n# Generate text\nwith torch.no_grad():\n    outputs = tuned_model.generate(\n        **inputs,\n        max_length=4096,  # Adjust max_length as needed\n        pad_token_id=tokenizer.eos_token_id,  # Ensure it pads correctly\n        eos_token_id=tokenizer.eos_token_id,\n        num_return_sequences=1\n    )\n\n# Decode the output, but only from the input length onwards\ngenerated_text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\nprint(generated_text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation on SuperGlue Score","metadata":{}},{"cell_type":"code","source":"\nprint(torch.cuda.device_count())  # Check number of GPUs available\nprint(torch.cuda.get_device_name(1))  # Check the name of the second GPU\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:07.028538Z","iopub.execute_input":"2024-10-24T11:55:07.028932Z","iopub.status.idle":"2024-10-24T11:55:07.034498Z","shell.execute_reply.started":"2024-10-24T11:55:07.028894Z","shell.execute_reply":"2024-10-24T11:55:07.033497Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"2\nTesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:08.625826Z","iopub.execute_input":"2024-10-24T11:55:08.626207Z","iopub.status.idle":"2024-10-24T11:55:21.485840Z","shell.execute_reply.started":"2024-10-24T11:55:08.626171Z","shell.execute_reply":"2024-10-24T11:55:21.484698Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:55:21.487879Z","iopub.execute_input":"2024-10-24T11:55:21.488198Z","iopub.status.idle":"2024-10-24T11:55:34.836813Z","shell.execute_reply.started":"2024-10-24T11:55:21.488162Z","shell.execute_reply":"2024-10-24T11:55:34.835808Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('super_glue', 'boolq') ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:57:17.596135Z","iopub.execute_input":"2024-10-24T11:57:17.596599Z","iopub.status.idle":"2024-10-24T11:57:24.954351Z","shell.execute_reply.started":"2024-10-24T11:57:17.596540Z","shell.execute_reply":"2024-10-24T11:57:24.953404Z"},"trusted":true},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/30.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b2f50ed807e4a89ad1ac774a2afe8b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/18.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888b2440236745db9e9eb5ae455146a0"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10fe47b6590b4afeaa54bb471cd9f518"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4503ece71e0e4318b4028a4175a22a94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3270 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68fbfe76776240f3a67831d9d24cbba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3245 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b360fff202a141b892273d44e365592a"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport evaluate\n\n# Load the BoolQ dataset and accuracy metric\nboolq_dataset = load_dataset('super_glue', 'boolq', split='validation')\naccuracy_metric = evaluate.load('accuracy')\n\n# Ensure the model is on the appropriate device (CUDA if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = tuned_model.to(device)\nmodel.eval()\n\n# Initialize list to store predictions\npredictions = []\n\n# Evaluate the model\nfor example in boolq_dataset:\n    # Extract the input text (this depends on the dataset field used)\n    input_text = example['passage'] if 'passage' in example else example['question']\n\n    # Tokenize the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    device = torch.device(\"cuda:0\")\n    # Run the model\n    with torch.no_grad():\n        outputs = model(**inputs).to(device)\n    \n    # Extract the predicted label (this depends on the model's output format)\n    predicted_label = torch.argmax(outputs.logits, dim=-1).item()\n    predictions.append(predicted_label)\n\n# Calculate accuracy\naccuracy = accuracy_metric.compute(predictions=predictions, references=boolq_dataset['label'])\nprint(f\"BoolQ accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:59:06.682281Z","iopub.execute_input":"2024-10-24T11:59:06.683045Z","iopub.status.idle":"2024-10-24T11:59:08.205635Z","shell.execute_reply.started":"2024-10-24T11:59:06.683000Z","shell.execute_reply":"2024-10-24T11:59:08.204187Z"},"trusted":true},"execution_count":64,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Extract the predicted label (this depends on the model's output format)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:891\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    888\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n\u001b[1;32m    894\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error"}]},{"cell_type":"markdown","source":"# ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","metadata":{}},{"cell_type":"code","source":"passage = '''This chapter deals with one of the most difficult challenges faced by independent India—poverty. After discussing this multi-dimensional problem through examples, the chapter discusses the way poverty is seen in social sciences. Poverty trends in India and the world are illustrated through the concept of the poverty line. Causes of poverty as well as anti-poverty measures taken by the government are also discussed. The chapter ends with broadening the official concept of poverty into human poverty. In our daily life, we come across many people who we think are poor. They could be landless labourers in villages or people living in overcrowded jhuggis in cities. They could be daily wage workers at construction sites or child workers in dhabas. They could also be beggars with children in tatters. We see poverty all around us. In fact, every fifth person in India is poor. (This means, roughly 270 million (or 27 crore) people in India live in poverty 2011-12.) This also means that India has the largest single concentration of the poor in the world. This illustrates the seriousness of the challenge. Urban Case Thirty-three year old Ram Saran works as a daily-wage labourer in a wheat flour mill near Ranchi in Jharkhand. He manages to earn around Rs 1,500 a month when he finds employment, which is not often. The money is not enough to sustain his family of six— that includes his wife and four children aged between 12 years to six months. He has to send money home to his old parents who live in a village near Ramgarh. His father a landless labourer, depends on Ram Saran and his brother who lives in Hazaribagh, for sustenance. Ram Saran lives in a one-room rented house in a crowded basti in the outskirts of the city. It’s a\ntemporary shack built of bricks and clay tiles. His wife Santa Devi, works as a part time maid in a few houses and manages to earn another Rs 800. They manage a meagre meal of dal and\nrice twice a day, but there’s never enough for all of them. His elder son works as a helper in a tea shop to supplement the family income and earns another Rs 300, while his 10- year-old daughter takes care of the younger siblings. None of the children go to school. They have only two pairs of hand-me-down clothes each. New ones are bought only when the old clothes become unwearable. Shoes are a luxury. The younger kids are undernourished. They have no access to healthcare when they fall ill. Rural case Lakha Singh belongs to a small village near Meerut in Uttar Pradesh. His family doesn’t own any land, so they do odd jobs for the big farmers. Work is erratic and so is income. At times they get paid Rs 50 for a hard day’s work. But often it’s in kind like a few kilograms of wheat or dal or even vegetables for toiling in the farm through the day. The family of eight cannot always manage two square meals a day. Lakha lives in a kuchha hut on the outskirts of the village. The women of the family spend the day chopping fodder and collecting firewood in the fields. His father a TB patient, passed away two years ago due to lack of medication. His mother now suffers from the same disease and life is slowly ebbing away.Although, the village has a primary school, Lakha never went there. He had to start earning when he was 10 years old. New clothes happen once in a few years. Even soap and oil are a luxury for the family. These two typical cases illustrate many dimensions of poverty. They show that poverty means hunger and lack of shelter. It also is a situation in which parents are\nnot able to send their children to school or a situation where sick people cannot afford treatment. Poverty also means lack of clean water and sanitation facilities. It also means lack of a regular job at a minimum decent level. Above all it means living with a sense of helplessness. Poor people are in a situation in which they are ill-treated at almost every place, in farms, factories, government offices, hospitals, railway stations etc. Obviously, nobody would like to live in poverty. One of the biggest challenges of independent India has been to bring millions of its people out of abject poverty. Mahatama Gandhi always insisted that India would be truly independent only when the poorest of its people become free of human suffering. Poverty as seen by social scientists Since poverty has many facets, social scientists look at it through a variety of indicators. Usually the indicators used relate to the levels of income and consumption. But now poverty is looked through other social indicators like illiteracy level, lack of general resistance due to malnutrition, lack of access to healthcare, lack of job opportunities, lack\nof access to safe drinking water, sanitation etc. Analysis of poverty based on social exclusion and vulnerability is now becoming very common.Social exclusion According to this concept, poverty must be seen in terms of the poor having to live only in a poor surrounding with other poor people, excluded from enjoying social equality of better-off people in better surroundings. Social exclusion can be both a cause as well as a consequence of poverty in the usual sense. Broadly, it is a process through which individuals or groups are excluded from facilities, benefits and opportunities that others (their “betters”) enjoy. A typical example is the working of the caste system in India in which people belonging to certain castes are excluded from equal opportunities. Social exclusion thus may lead to, but can cause more damage than, having a very low income. Vulnerability to poverty is a measure, which describes the greater\nprobability of certain communities (say, members of a backward caste) or individuals (such as a widow or a physically handicapped person) of becoming, or remaining, poor in the coming years. Vulnerability is determined by the options available to different communities for finding\nan alternative living in terms of assets, education, health and job opportunities. Further, it is analysed on the basis of the greater risks these groups face at the time of natural disasters (earthquakes, tsunami), terrorism etc. Additional analysis is made of their social and economic\nability to handle these risks. In fact, vulnerability describes the greater probability of being more adversely affected than other people when bad time comes for everybody, whether a\nflood or an earthquake or simply a fall in the availability of jobs!\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}